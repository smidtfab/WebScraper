{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse as parse\n",
    "import urllib.robotparser as rp\n",
    "\n",
    "from socket import gethostbyname, gaierror\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 50     # max nb of links to crawl\n",
    "max_cont = 20      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one function to retrieve all external links in the page (links to the same domain do not begin by htts, and as such are not spotted by Beautiful Soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links_robot(url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        print(\"Counter = {} - max_l = {}\".format(counter, max_l))\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            save_link_robot(link.get('href'), str(counter))\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-236dad59f28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinks_crawled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_links\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_crawled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_crawled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "links_crawled, counter = get_all_links (url, counter)\n",
    "print(len(links_crawled))\n",
    "print(links_crawled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function saves the link under a specified filename. The path for that file can be changed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link(url, filename):\n",
    "    doc = requests.get(url)\n",
    "    \n",
    "    # Put here the path where you want to save the link's contents\n",
    "    name = \"Docs_crawled_noRobot/\" + filename + \".html\"\n",
    "    \n",
    "    with open(name, 'wb') as fOut:\n",
    "        fOut.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link_robot(url, filename):\n",
    "    try:\n",
    "        doc = requests.get(url.strip())\n",
    "        # Put here the path where you want to save the link's contents\n",
    "        name = \"docs_crawled/\" + filename + \".html\"\n",
    "        with open(name, 'wb') as fOut:\n",
    "            fOut.write(doc.content)\n",
    "    except gaierror:\n",
    "        print(\"Link not reachable\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'0001'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to have each file's name be its index in the list of links, on 4 characters (for instance: 0008 for the 9th link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare new filename\n",
    "def new_filename (count, fname = '0000', conv = [3, 1]):\n",
    "    nb_zeros = conv[0]\n",
    "    pw = conv[1]\n",
    "    \n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count)\n",
    "    \n",
    "    return (filename, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'links_crawled' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-61bbf609cb01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks_crawled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Save the link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msave_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'links_crawled' is not defined"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "\n",
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "\n",
    "conventions = [nb_zeros, pw]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    filename, conventions = new_filename(count, filename, conventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling without considering robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method return at most max_c links and saves at most max_c, starting from an initial url and getting its children links, their children links and so on in a breadth-first fashion.   \n",
    "\n",
    "Returns:\n",
    "    list - a list of strings that are the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler_noRespect(L = [URL], pointer=0, last_expanded = 0, fname = '0000', \\\n",
    "                      max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    url = L[pointer]\n",
    "\n",
    "    # Save contents of the link\n",
    "    save_link(url, fname)\n",
    "    max_c -= 1                 \n",
    "\n",
    "    # Prepare filename for next page\n",
    "    pointer += 1\n",
    "    fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "    # If we do not have enough links, save those referenced in the page\n",
    "    if (len(L) < max_l):\n",
    "        L, counter = get_all_links(url, counter, L, max_l)\n",
    "        last_expanded +=1\n",
    "    \n",
    "    # If we are close to the end of the queue of links and have not saved enough content yet, get more \n",
    "    if (pointer >= (len(L) - 2)):\n",
    "        l = len(L)\n",
    "        while (len(L) == l):\n",
    "            L, counter2 = get_all_links(L[last_expanded], 0, L, max_l)\n",
    "            last_expanded +=1\n",
    "            counter += counter2\n",
    "\n",
    "    # If we have not saved enough links: repeat\n",
    "    if (max_c > 0):\n",
    "        L = Crawler_noRespect (L, pointer, last_expanded, fname, max_l, max_c, conv)\n",
    "    \n",
    "    return (L)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "L_noRobots = Crawler_noRespect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt permissions and crawler delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save lists of domains, robots.txt urls, crawler delays and times when each url's domain was last crawled. The information on the same url will be at the same index i in all lists (i.e. in line i of the matrix D). We then use these to wait the correct amount of time before crawling each domain a second time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "robots = []\n",
    "delays = []\n",
    "time_crawled = []\n",
    "\n",
    "Links = [URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with D as list of lists (domains, robots, delay, time_crawled)\n",
    "\n",
    "def Crawler (D = [domains, robots, delays, time_crawled], L=Links, pointer=0, \\\n",
    "             fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    #print(\"Nb links in Links: \", counter)\n",
    "    #print(\"Pointer: \", pointer)\n",
    "    already_crawled = True\n",
    "    \n",
    "    # Issue with some links: don't parse them        \n",
    "    #if not (pointer in [1, 4]):\n",
    "    \n",
    "    url = L[pointer]\n",
    "    print(url)\n",
    "\n",
    "    # Get url domain\n",
    "    URL_parsed = parse.urlparse(url)\n",
    "    dom = URL_parsed.netloc\n",
    "    #print(\"Domain: \", dom)\n",
    "\n",
    "    # If we have not already tried to crawl in the domain\n",
    "    if not (dom in domains):\n",
    "\n",
    "        #print(\"New domain\")\n",
    "        already_crawled = False        \n",
    "        # Get robots.txt link\n",
    "        r = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "        #print(\"Robots.txt link: \", r)\n",
    "\n",
    "        domains.append(dom)\n",
    "        robots.append(r)\n",
    "        delays.append(20)\n",
    "        time_crawled.append(-1)\n",
    "\n",
    "\n",
    "    i = domains.index(dom)\n",
    "    #print(\"Domain index: \", i)\n",
    "    r = robots[i]\n",
    "    #print(\"Robots.txt link: \", r)\n",
    "\n",
    "    # If our domain is not dead (xocoatl refuses all robots)\n",
    "    if (not dom in [\"www.cfsan.fda.gov\", \"www.cocoatree.org\", \"nca.files.cms-plus.com\", \\\n",
    "                    \"www.xocoatl.org\", \"www.cfsan.fda.gov\"]):\n",
    "        \n",
    "        # Access robots.txt\n",
    "        RP = rp.RobotFileParser()         \n",
    "        RP.set_url(r) \n",
    "        RP.read()\n",
    "\n",
    "        # If the robot parser correctly opens the robots.txt. \n",
    "        # This test is actually too harsh, but it's better to not parse some links that could be saved rather than \n",
    "        # stop the whole process (some robots.txt links actually send us directly to the home domain first, or don't exist)\n",
    "        if (len(RP.entries) != 0):\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if ((not already_crawled) & (RP.crawl_delay(\"*\") != None)):\n",
    "                delays.pop(i) \n",
    "                delays.insert(i, RP.crawl_delay(\"*\"))\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", url)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                if (already_crawled):\n",
    "                    wait = max(0, delays[i] - (time.time() - time_crawled[i]))\n",
    "                    if (wait > 0): \n",
    "                               time.sleep(wait)\n",
    "\n",
    "                \n",
    "                max_c -= 1\n",
    "                print(\"\\n MAX_C - > {}\".format(max_c))\n",
    "                print(\"We are saving the contents of \", url)\n",
    "\n",
    "                # Update time we last crawled the domain\n",
    "                time_crawled.pop(i)\n",
    "                time_crawled.insert(i, time.time())                  \n",
    "\n",
    "                # Prepare filename for next page\n",
    "                fname, conv = new_filename(pointer + 1, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referenced in the page\n",
    "                if (len(L) < max_l):\n",
    "                    L, counter = get_all_links_robot(L[pointer], counter, L, max_l)\n",
    "    print(D)\n",
    "    #print(L)\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (counter < max_l):\n",
    "        pointer += 1\n",
    "        print(\"COUNTER_IF = {} - max_l = {}, pointer = {}\".format(counter, max_l, pointer))\n",
    "        D, L = Crawler (D, L, pointer, fname, max_l, max_c, conv)\n",
    "    else:\n",
    "        return (D, L)\n",
    "    \n",
    "    return (D, L)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://en.wikipedia.org/wiki/Chocolate\n\n MAX_C - > 19\nWe are saving the contents of  https://en.wikipedia.org/wiki/Chocolate\nCounter = 1 - max_l = 50\nCounter = 2 - max_l = 50\nCounter = 3 - max_l = 50\nCounter = 4 - max_l = 50\nCounter = 5 - max_l = 50\nCounter = 6 - max_l = 50\nCounter = 7 - max_l = 50\nCounter = 8 - max_l = 50\nCounter = 9 - max_l = 50\nCounter = 10 - max_l = 50\nCounter = 11 - max_l = 50\nCounter = 12 - max_l = 50\nCounter = 13 - max_l = 50\nCounter = 14 - max_l = 50\nCounter = 15 - max_l = 50\nCounter = 16 - max_l = 50\nCounter = 17 - max_l = 50\nCounter = 18 - max_l = 50\nCounter = 19 - max_l = 50\nCounter = 20 - max_l = 50\nCounter = 21 - max_l = 50\nConnection Error\nCounter = 22 - max_l = 50\nCounter = 23 - max_l = 50\nCounter = 24 - max_l = 50\nCounter = 25 - max_l = 50\nCounter = 26 - max_l = 50\nCounter = 27 - max_l = 50\nCounter = 28 - max_l = 50\nCounter = 29 - max_l = 50\nCounter = 30 - max_l = 50\nCounter = 31 - max_l = 50\nCounter = 32 - max_l = 50\nCounter = 33 - max_l = 50\nCounter = 34 - max_l = 50\nCounter = 35 - max_l = 50\nCounter = 36 - max_l = 50\nCounter = 37 - max_l = 50\nCounter = 38 - max_l = 50\nCounter = 39 - max_l = 50\nConnection Error\nCounter = 40 - max_l = 50\nCounter = 41 - max_l = 50\nCounter = 42 - max_l = 50\nCounter = 43 - max_l = 50\nCounter = 44 - max_l = 50\nCounter = 45 - max_l = 50\nCounter = 46 - max_l = 50\nConnection Error\nCounter = 47 - max_l = 50\nCounter = 48 - max_l = 50\nCounter = 49 - max_l = 50\nCounter = 50 - max_l = 50\n[['en.wikipedia.org'], ['https://en.wikipedia.org/robots.txt'], [20], [1584731196.2920458]]\n"
    }
   ],
   "source": [
    "# Emptying lists from previous tests\n",
    "for l in [domains, robots, delays, time_crawled]:\n",
    "    while len(l) > 0:\n",
    "            l.pop(0)\n",
    "\n",
    "while (len(Links) > 1):\n",
    "    Links.pop(1)\n",
    "\n",
    "# Save contents of the root link\n",
    "save_link_robot(URL, \"000\")\n",
    "D, Links = Crawler()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}