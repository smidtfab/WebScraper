{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse as parse\n",
    "import urllib.robotparser as rp\n",
    "\n",
    "from socket import gethostbyname, gaierror\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from socket import gethostbyname, gaierror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links as often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 80     # max nb of links to crawl\n",
    "max_cont = 10      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one function to retrieve all external links in the page (links to the same domain do not begin by htts, and as such are not spotted by Beautiful Soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        print(\"Counter = {} - max_l = {}\".format(counter, max_l))\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            save_link(link.get('href'), str(counter))\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    counter = len(Links)\n",
    "        \n",
    "    Links, counter = get_all_links (url, counter, Links, max_l)\n",
    "    \n",
    "    # If we still do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n50\n"
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link (url, filename):\n",
    "    try:\n",
    "        doc = requests.get(url.strip())\n",
    "        # Put here the path where you want to save the link's contents\n",
    "        name = \"Docs_crawled4/\" + filename + \".html\"\n",
    "        with open(name, 'wb') as fOut:\n",
    "            fOut.write(doc.content)\n",
    "    except gaierror:\n",
    "        print(\"Link not reachable\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'0001'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convention: the filename will be the url's index in the list, on 4 characters\n",
    "\n",
    "# Prepare new filename\n",
    "def new_filename (count, fname = '0000', conv = [3, 1]):\n",
    "    nb_zeros = conv[0]\n",
    "    pw = conv[1]\n",
    "    \n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count)\n",
    "    \n",
    "    return (filename, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "\n",
    "conventions = [nb_zeros, pw]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    filename, conventions = new_filename(count, filename, conventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling without considering robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler_noRespect(L = [URL], pointer=0, last_expanded = 0, fname = '0000', \\\n",
    "                      max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    url = L[pointer]\n",
    "\n",
    "    # Save contents of the link\n",
    "    save_link(url, fname)\n",
    "    max_c -= 1                 \n",
    "\n",
    "    # Prepare filename for next page\n",
    "    pointer += 1\n",
    "    fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "    # If we do not have enough links, save those referenced in the page\n",
    "    if (len(L) < max_l):\n",
    "        L, counter = get_all_links(url, counter, L, max_l)\n",
    "    \n",
    "    # If we are close to the end of the queue of links and have not saved enough content yet, get more \n",
    "    if (pointer >= (len(L) - 2)):\n",
    "        l = len(L)\n",
    "        while (len(L) == l):\n",
    "            last_expanded +=1\n",
    "            L, counter2 = get_all_links(L[last_expanded], 0, L, max_l)\n",
    "            counter += counter2\n",
    "\n",
    "    # If we have not saved enough links: repeat\n",
    "    if (max_c > 0):\n",
    "        L = Crawler_noRespect (L, pointer, last_expanded, fname, max_l, max_c, conv)\n",
    "    \n",
    "    return (L)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: works 100% fine\n",
    "\n",
    "L_noRobots = Crawler_noRespect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "en.wikipedia.org\nhttps://en.wikipedia.org/robots.txt\n"
    }
   ],
   "source": [
    "# Test on URL = wikipedia\n",
    "\n",
    "# Getting domain url and robots.txt link\n",
    "## Using urlparse\n",
    "URL_parsed = parse.urlparse(URL)\n",
    "print(URL_parsed.netloc)\n",
    "URL_parsed\n",
    "\n",
    "robot = URL_parsed.scheme + \"://\" + URL_parsed.netloc + \"/robots.txt\"\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://en.wikipedia.org/wiki/robots.txt\nhttp://www.eluniversal.com.mx/notas/robots.txt\n"
    }
   ],
   "source": [
    "## Using urljoin -DOESN'T WORK \n",
    "robot = parse.urljoin(URL, \"robots.txt\")\n",
    "print(robot)\n",
    "\n",
    "robot = parse.urljoin(links_crawled[2], \"robots.txt\")\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Robot parsing\n",
    "\n",
    "RP = rp.RobotFileParser()            # Define Robot parser object\n",
    "\n",
    "#robot = \"https://en.wikipedia.org/robots.txt\"\n",
    "RP.set_url(robot)                    # Set robots.txt link\n",
    "\n",
    "RP.read()                            # Open robots.txt file\n",
    "rrate = RP.request_rate(\"*\")         # Return requests per second rate (requests, seconds)\n",
    "print(RP.crawl_delay(\"*\"))           # Returns crawl delay\n",
    "\n",
    "RP.can_fetch(\"*\", URL)               # Returns bool if can crawl that url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if robots.txt exists -> doesn't work for our second link\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            print(1)\n",
    "            return True\n",
    "        else:\n",
    "            print(0)\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        # handle your exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function bringing it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode: Crawler (nb_max_url, [URL], Domains, Links):\n",
    "\n",
    "Get robots.txt url + domain \n",
    "Save domain, robots.txt url and crawl delay if don't already have it\n",
    "\n",
    "If can fetch: \n",
    "\n",
    "    If domain in domains & time crawled > 0:\n",
    "        Wait max(0, crawl delay - (current time - time crawled))\n",
    "    \n",
    "    Crawl: save contents \n",
    "    Update time crawled\n",
    "    Save links if we still need more\n",
    "    Save in matrix domains line: domain, crawl delay (20ms if none), time crawled\n",
    "\n",
    "If haven't crawled enough links: repeat\n",
    "    \n",
    "Links = list of links we can crawl\n",
    "Domains: [domain, robots.txt link, crawl delay, time last crawled]\n",
    "(several links can have same domain)\n",
    "\n",
    "Auxiliary functions: extract up to M links from a page, new filename, save a link (all checked individually, work)\n",
    "\n",
    "To be able to wait for crawler delay time, need functions get current time, wait a certain time (need to know units of said time!!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links_robot (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links_robot(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    counter = len(Links)\n",
    "        \n",
    "    Links, counter = get_all_links_robot (url, counter, Links, max_l)\n",
    "            \n",
    "    # If we still do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links_robot(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n50\n"
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function with Domain matrix as list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "robots = []\n",
    "delays = []\n",
    "time_crawled = []\n",
    "\n",
    "Links = [URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with D as list of lists (domains, robots, delay, time_crawled)\n",
    "\n",
    "def Crawler (D = [domains, robots, delays, time_crawled], L=Links, pointer=0, last_expanded = 0, \\\n",
    "             fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    #print(\"Nb links in Links: \", counter)\n",
    "    #print(\"Pointer: \", pointer)\n",
    "    already_crawled = True\n",
    "    \n",
    "    # Issue with some links: don't parse them        \n",
    "    #if not (pointer in [1, 4]):\n",
    "    \n",
    "    url = L[pointer]\n",
    "    print(url)\n",
    "\n",
    "    # Get url domain\n",
    "    URL_parsed = parse.urlparse(url)\n",
    "    dom = URL_parsed.netloc\n",
    "    #print(\"Domain: \", dom)\n",
    "\n",
    "    # If we have not already tried to crawl in the domain\n",
    "    if not (dom in domains):\n",
    "\n",
    "        #print(\"New domain\")\n",
    "        already_crawled = False        \n",
    "        # Get robots.txt link\n",
    "        r = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "        #print(\"Robots.txt link: \", r)\n",
    "\n",
    "        domains.append(dom)\n",
    "        robots.append(r)\n",
    "        delays.append(20)\n",
    "        time_crawled.append(-1)\n",
    "\n",
    "\n",
    "    i = domains.index(dom)\n",
    "    #print(\"Domain index: \", i)\n",
    "    r = robots[i]\n",
    "    #print(\"Robots.txt link: \", r)\n",
    "\n",
    "    # If our domain is not dead (xocoatl refuses all robots)\n",
    "    if (not dom in [\"www.cfsan.fda.gov\", \"www.cocoatree.org\", \"nca.files.cms-plus.com\", \\\n",
    "                    \"www.xocoatl.org\", \"www.cfsan.fda.gov\"]):\n",
    "        \n",
    "        # Access robots.txt\n",
    "        RP = rp.RobotFileParser()         \n",
    "        RP.set_url(r) \n",
    "        RP.read()\n",
    "\n",
    "        # If the robot parser correctly opens the robots.txt. \n",
    "        # This test is actually too harsh, but it's better to not parse some links that could be saved rather than \n",
    "        # stop the whole process (some robots.txt links actually send us directly to the home domain first, or don't exist)\n",
    "        if (len(RP.entries) != 0):\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if ((not already_crawled) & (RP.crawl_delay(\"*\") != None)):\n",
    "                delays.pop(i) \n",
    "                delays.insert(i, RP.crawl_delay(\"*\"))\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", url)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                if (already_crawled):\n",
    "                    wait = max(0, delays[i] - (time.time() - time_crawled[i]))\n",
    "                    if (wait > 0): \n",
    "                               time.sleep(wait)\n",
    "\n",
    "                # Save contents of the link\n",
    "                save_link(url, fname)\n",
    "                max_c -= 1\n",
    "                print()\n",
    "                print(\"\\n MAX_C - > {}\".format(max_c))\n",
    "                print(\"We are saving the contents of \", url)\n",
    "\n",
    "                # Update time we last crawled the domain\n",
    "                time_crawled.pop(i)\n",
    "                time_crawled.insert(i, time.time())                  \n",
    "\n",
    "                # Prepare filename for next page\n",
    "                fname, conv = new_filename(pointer + 1, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referenced in the page\n",
    "                if (len(L) < max_l):\n",
    "                    L, counter = get_all_links(L[pointer], counter, L, max_l)\n",
    "                    last_expanded += 1\n",
    "    print(D)\n",
    "    #print(L)\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (counter < max_l):\n",
    "        pointer += 1\n",
    "        print(\"COUNTER_IF = {} - max_l = {}, pointer = {}\".format(counter, max_l, pointer))\n",
    "        D, L = Crawler (D, L, pointer, last_expanded, fname, max_l, max_c, conv)\n",
    "    else:\n",
    "        return (D, L)\n",
    "    \n",
    "    return (D, L)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://en.wikipedia.org/wiki/Chocolate\n\n\n MAX_C - > 9\nWe are saving the contents of  https://en.wikipedia.org/wiki/Chocolate\nCounter = 1 - max_l = 80\nCounter = 2 - max_l = 80\nCounter = 3 - max_l = 80\nCounter = 4 - max_l = 80\nCounter = 5 - max_l = 80\nCounter = 6 - max_l = 80\nCounter = 7 - max_l = 80\nCounter = 8 - max_l = 80\nCounter = 9 - max_l = 80\nCounter = 10 - max_l = 80\nCounter = 11 - max_l = 80\nCounter = 12 - max_l = 80\nCounter = 13 - max_l = 80\nCounter = 14 - max_l = 80\nCounter = 15 - max_l = 80\nCounter = 16 - max_l = 80\nCounter = 17 - max_l = 80\nCounter = 18 - max_l = 80\nCounter = 19 - max_l = 80\nCounter = 20 - max_l = 80\nCounter = 21 - max_l = 80\nConnection Error\nCounter = 22 - max_l = 80\nCounter = 23 - max_l = 80\nCounter = 24 - max_l = 80\nCounter = 25 - max_l = 80\nCounter = 26 - max_l = 80\nCounter = 27 - max_l = 80\nCounter = 28 - max_l = 80\nCounter = 29 - max_l = 80\nCounter = 30 - max_l = 80\nCounter = 31 - max_l = 80\nCounter = 32 - max_l = 80\nCounter = 33 - max_l = 80\nCounter = 34 - max_l = 80\nCounter = 35 - max_l = 80\nCounter = 36 - max_l = 80\nCounter = 37 - max_l = 80\nCounter = 38 - max_l = 80\nCounter = 39 - max_l = 80\nConnection Error\nCounter = 40 - max_l = 80\nCounter = 41 - max_l = 80\nCounter = 42 - max_l = 80\nCounter = 43 - max_l = 80\nCounter = 44 - max_l = 80\nCounter = 45 - max_l = 80\nCounter = 46 - max_l = 80\nConnection Error\nCounter = 47 - max_l = 80\nCounter = 48 - max_l = 80\nCounter = 49 - max_l = 80\nCounter = 50 - max_l = 80\nCounter = 51 - max_l = 80\nCounter = 52 - max_l = 80\nCounter = 53 - max_l = 80\nCounter = 54 - max_l = 80\nCounter = 55 - max_l = 80\nCounter = 56 - max_l = 80\nCounter = 57 - max_l = 80\nCounter = 58 - max_l = 80\nCounter = 59 - max_l = 80\nCounter = 60 - max_l = 80\nCounter = 61 - max_l = 80\nCounter = 62 - max_l = 80\nCounter = 63 - max_l = 80\nCounter = 64 - max_l = 80\nCounter = 65 - max_l = 80\nConnection Error\nCounter = 66 - max_l = 80\nConnection Error\nCounter = 67 - max_l = 80\nCounter = 68 - max_l = 80\nCounter = 69 - max_l = 80\nCounter = 70 - max_l = 80\nCounter = 71 - max_l = 80\nCounter = 72 - max_l = 80\nConnection Error\nCounter = 73 - max_l = 80\nCounter = 74 - max_l = 80\nCounter = 75 - max_l = 80\nCounter = 76 - max_l = 80\nCounter = 77 - max_l = 80\nCounter = 78 - max_l = 80\n[['en.wikipedia.org'], ['https://en.wikipedia.org/robots.txt'], [20], [1584727982.5746922]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate\n[['en.wikipedia.org', 'ndb.nal.usda.gov'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt'], [20, 20], [1584727982.5746922, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://www.eluniversal.com.mx/notas/526113.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt'], [20, 20, 20], [1584727982.5746922, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt'], [20, 20, 20, 20], [1584727982.5746922, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://www.bartleby.com/61/68/C0316800.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt'], [20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://antiquity.ac.uk/projgall/powis/index.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt'], [20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://www.museum.upenn.edu/new/news/fullrelease.php?which=306\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://archive.fieldmuseum.org/chocolate/history.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 and max_l = 80\nhttp://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford\n\n\n MAX_C - > 8\nWe are saving the contents of  http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford\nCounter = 79 - max_l = 80\nCounter = 80 - max_l = 80\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [1584727982.5746922, -1, -1, -1, -1, -1, -1, -1, -1, 1584728235.2339888]]\n"
    }
   ],
   "source": [
    "# Emptying lists from previous tests\n",
    "for l in [domains, robots, delays, time_crawled]:\n",
    "    while len(l) > 0:\n",
    "            l.pop(0)\n",
    "\n",
    "while (len(Links) > 1):\n",
    "    Links.pop(1)\n",
    "\n",
    "D, Links = Crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n"
     ]
    }
   ],
   "source": [
    "print(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 5], ['aba', 'baa', 'daa', 'ett'], [20, 20, 20, 50], [1, 2.3, 2.4, 3]]\n",
      "['aba', 'gg', 'daa', 'ett']\n",
      "[1, 4, 2.4, 3]\n",
      "[[1, 2, 3, 5], ['aba', 'gg', 'daa', 'ett'], [20, 20, 20, 50], [1, 4, 2.4, 3]]\n"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains: [domains, robot_links, delay, time]\n",
    "\n",
    "domains = [1, 2, 3, 5]\n",
    "robot_links = [\"aba\", \"baa\", \"daa\", \"ett\"]\n",
    "delay = [20, 20, 20, 50]\n",
    "time = [1, 2.3, 2.4, 3]\n",
    "\n",
    "D = [domains, robot_links, delay, time]\n",
    "print(D)\n",
    "\n",
    "dom = 2\n",
    "\n",
    "# pop then insert method done directly in list and D\n",
    "if (dom in domains):\n",
    "    i = domains.index(dom)\n",
    "    \n",
    "    #robot_links[i] = \"g\"\n",
    "    robot_links.pop(i)\n",
    "    robot_links.insert(i, \"gg\")\n",
    "    print(robot_links)\n",
    "    \n",
    "    time.pop(i)\n",
    "    time.insert(i, 4)\n",
    "    print(time)\n",
    "    \n",
    "# append done directly in domains and D\n",
    "else:\n",
    "    i = len(domains)\n",
    "    domains.append(dom)\n",
    "    robot_links.append(\"c\")\n",
    "    delay.append(20)\n",
    "    time.append(-1)\n",
    "\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-ea69e73f2215>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-ea69e73f2215>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    D2 = D[][3]\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains: [domain 1, domain 2,...] -> Obsolete\n",
    "\n",
    "D = [[1, \"a\", 20, 1], [2, \"b\", 20, 2.3], [5, \"d\", 30, 2.4]]\n",
    "\n",
    "#i= None\n",
    "j = 0\n",
    "#while (not i):\n",
    "#    i = D[j].index(5)\n",
    "#    j += 1\n",
    "\n",
    "while (5 not in D[j]):\n",
    "    j += 1\n",
    "\n",
    "print(j)\n",
    "\n",
    "D2 = D[][3]\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function with Domains matrix as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with Domains as a pandas dataframe\n",
    "\n",
    "def Crawler_df (Domains = pd.DataFrame({\"domains\": [],\"robot_links\": [],\"crawl_delay\": [],\"time\": []}), Links=[URL], pointer=0, fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "          \n",
    "    url = Links[pointer]\n",
    "    already_crawled = True\n",
    "    counter = len(Links)\n",
    "    \n",
    "    if (pointer != 1):\n",
    "    \n",
    "        # Get url domain\n",
    "        URL_parsed = parse.urlparse(url)\n",
    "        dom = URL_parsed.netloc\n",
    "\n",
    "        # If we have not already tried to crawl in the domain\n",
    "        if (not dom in Domains[\"domains\"]):\n",
    "            already_crawled = False\n",
    "            # Get robots.txt link\n",
    "            robot = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "\n",
    "            D = pd.DataFrame ({\n",
    "                \"domains\": [dom],\n",
    "                \"robot_links\": [robot],\n",
    "                \"crawl_delay\": [20],\n",
    "                \"time\": [-1]\n",
    "            })        \n",
    "            Domains = pd.concat([Domains, D])\n",
    "            Domains.reset_index(drop=True, inplace=True)\n",
    "            print(Domains.head())\n",
    "\n",
    "        # Set i the index of the row containing our current domain\n",
    "        i = Domains.index[Domains[\"domains\"] == dom].tolist()[0]\n",
    "\n",
    "        robot = Domains.loc[i, \"robot_links\"]\n",
    "        print(robot)\n",
    "\n",
    "        # If robots.txt exists:\n",
    "        if (does_url_exist(robot)):\n",
    "\n",
    "            # Access robots.txt\n",
    "            RP = rp.RobotFileParser()         \n",
    "            RP.set_url(robot)  \n",
    "            RP.read()\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if (not already_crawled) & (RP.crawl_delay(\"*\") != None):\n",
    "                Domains.loc[i, \"crawl_delay\"] = RP.crawl_delay(\"*\")\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", URL)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                #if (already_crawled):\n",
    "                #    Wait max(0, crawl delay - (current time - time crawled))\n",
    "                # Use time.sleep(time to wait), first import time\n",
    "                # time.time() = get_cpu time now\n",
    "\n",
    "                # Save contents of the link\n",
    "                save_link(url, fname)\n",
    "                # Domains.loc[i, \"time\"] = current_time                  # update time we last crawled the domain\n",
    "\n",
    "                # Prepare filename for next page\n",
    "                pointer += 1\n",
    "                fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referencec in the page\n",
    "                if len(Links) < max_l:\n",
    "                    Links, counter = get_all_links(url, counter, Links, max_l)\n",
    "\n",
    "                print(pointer)\n",
    "                \n",
    "    else: \n",
    "        pointer +=1\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (pointer < max_c):\n",
    "        Domains, Links = Crawler_df (Domains, Links, pointer, fname, max(0, max_l - counter), (max_c - pointer), conv)\n",
    "        \n",
    "    return (Domains, Links)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            domains                          robot_links  crawl_delay  time\n",
      "0  en.wikipedia.org  https://en.wikipedia.org/robots.txt         20.0  -1.0\n",
      "https://en.wikipedia.org/robots.txt\n",
      "1\n",
      "1\n",
      "            domains                          robot_links  crawl_delay  time\n",
      "0  en.wikipedia.org  https://en.wikipedia.org/robots.txt         20.0  -1.0\n",
      "1      www.pnas.org       http://www.pnas.org/robots.txt         20.0  -1.0\n",
      "http://www.pnas.org/robots.txt\n",
      "1\n",
      "3\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                  domains                               robot_links  \\\n",
       " 0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
       " 1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
       " 2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 5  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 6  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " \n",
       "    crawl_delay  time  \n",
       " 0         20.0  -1.0  \n",
       " 1         10.0  -1.0  \n",
       " 2         20.0  -1.0  \n",
       " 3         20.0  -1.0  \n",
       " 4         20.0  -1.0  \n",
       " 5         20.0  -1.0  \n",
       " 6         20.0  -1.0  ,\n",
       " ['https://en.wikipedia.org/wiki/Chocolate',\n",
       "  'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate',\n",
       "  'http://www.pnas.org/content/108/21/8595',\n",
       "  'http://www.eluniversal.com.mx/notas/526113.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html',\n",
       "  'http://www.bartleby.com/61/68/C0316800.html',\n",
       "  'http://antiquity.ac.uk/projgall/powis/index.html',\n",
       "  'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america',\n",
       "  'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html',\n",
       "  'http://archive.fieldmuseum.org/chocolate/history.html',\n",
       "  'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999',\n",
       "  'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford',\n",
       "  'http://www.exploratorium.edu/exploring/exploring_chocolate/',\n",
       "  'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist',\n",
       "  'http://hdl.handle.net/2027/spo.did2222.0000.474',\n",
       "  'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate',\n",
       "  'http://www.bartleby.com/61/68/C0316800.html',\n",
       "  'http://www.etymonline.com/?term=Chocolate',\n",
       "  'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT',\n",
       "  'http://www.cfsan.fda.gov/~lrd/fr021004.html',\n",
       "  'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf',\n",
       "  'http://www.spiritofaloha.com/features/0907/rarest.html',\n",
       "  'http://news.bbc.co.uk/2/hi/africa/2042474.stm',\n",
       "  'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/',\n",
       "  'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt',\n",
       "  'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml',\n",
       "  'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf',\n",
       "  'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78',\n",
       "  'http://www.businessinsider.com/when-chocolate-extinct-2017-12',\n",
       "  'http://www.botanical.com/botanical/mgmh/c/cacao-02.html',\n",
       "  'http://www.xocoatl.org/tree.htm',\n",
       "  'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm',\n",
       "  'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html',\n",
       "  'http://www.sensationalchocolates.com/ingredients.html',\n",
       "  'http://www.icco.org/questions/varieties.htm',\n",
       "  'http://www.xocoatl.org/harvest.htm',\n",
       "  'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf',\n",
       "  'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp',\n",
       "  'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf',\n",
       "  'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf',\n",
       "  'http://news.bbc.co.uk/1/hi/uk/678141.stm',\n",
       "  'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66',\n",
       "  'http://www.sallys-place.com/food/columns/zonis/conching.htm',\n",
       "  'http://candy.about.com/od/candybasics/ht/temperchoc.htm',\n",
       "  'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf',\n",
       "  'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf',\n",
       "  'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crawler_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   domains robots_link  crawl_delay  time\n",
      "0        1           a           20   1.0\n",
      "1        2           b           20   2.3\n",
      "2        3           d           20   2.4\n",
      "3        5           e           50   3.0\n",
      "2    d\n",
      "Name: robots_link, dtype: object\n",
      "2    d\n",
      "Name: robots_link, dtype: object\n",
      "2\n",
      "i\n",
      "   domains robots_link  crawl_delay  time\n",
      "0        1           a           20   1.0\n",
      "1        2           b           20   2.3\n",
      "2        3           i           20   2.4\n",
      "3        5           e           50   3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clothilde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-8d6cb89889c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"domains\"\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "# Test dataframe methods on Domains \n",
    "\n",
    "D = pd.DataFrame ({\n",
    "    \"domains\": [1, 2, 3, 5],\n",
    "    \"robots_link\": [\"a\", \"b\", \"d\", \"e\"],\n",
    "    \"crawl_delay\": [20, 20, 20, 50],\n",
    "    \"time\": [1, 2.3, 2.4, 3]\n",
    "})\n",
    "\n",
    "print(D.head())\n",
    "\n",
    "#if 10 not in D[\"domains\"]:\n",
    "#    pd.melt(D, [10, \"g\", 20, 3.2]) #=> Melt only with df \n",
    "#print(D.head())\n",
    "\n",
    "dom = 3\n",
    "\n",
    "curr = D.loc[D[\"domains\"] == 3]   # Makes copy of row!!! Can't modif that\n",
    "\n",
    "#i = D[D[\"domains\"] == 3].index\n",
    "#print(i)\n",
    "#D[i, 2] = 10\n",
    "#print(D.head())\n",
    "\n",
    "#r = D[D[\"domains\"] == dom]\n",
    "#print(r)\n",
    "#print(r[\"time\"])\n",
    "\n",
    "r = D[D[\"domains\"] == dom][\"robots_link\"]\n",
    "print(r)\n",
    "\n",
    "D[D[\"domains\"] == dom][\"robots_link\"] = \"h\"\n",
    "print(D[D[\"domains\"] == dom][\"robots_link\"])\n",
    "\n",
    "i = D.index[D[\"domains\"] == dom].tolist()[0]\n",
    "print(i)\n",
    "\n",
    "D.loc[i, \"robots_link\"] = \"i\"\n",
    "print(D.loc[i, \"robots_link\"])\n",
    "print(D.head())\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}