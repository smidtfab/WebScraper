{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import deque\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 50     # max nb of links to crawl\n",
    "max_cont = 20      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    counter = len(Links)\n",
    "\n",
    "    # Use regex to extract all links from url, stop if we reach maximum\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # If we stll do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.pnas.org/content/108/21/8595', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link (url, filename):\n",
    "    doc = requests.get(url)\n",
    "    name = \"Documents_crawled/\" + filename + \".html\"\n",
    "    with open(name, 'wb') as fOut:\n",
    "        fOut.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convention: the filename will be the url's index in the list, on 4 characters\n",
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    # Prepare new filename\n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use library robot-parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code:\n",
    "\n",
    "l_servers: list of (server name, crawl_delay) (where crawl_delay = 20ms by default)\n",
    "\n",
    "if link's server is in list: wait for (crawl_delay time)\n",
    "\n",
    "if (\"User-agent: \\*\" not in robots.txt): get full contents\n",
    "else:\n",
    "    line = 1st line under \"User-agent: \\*\"\n",
    "    if (\"User agent:\\* \\n Allow: \\\" in robots.txt; or \"Allow: \\\" in line): get full contents of link\n",
    "    if (\"User agent:\\* \\n Disallow: \\\" in robots.txt; or \"Disallow: \\\" in line): break\n",
    "    else:\n",
    "        while (\"Allow\" in line) | (\"Disallow\" in line):\n",
    "            if \"Allow\" in line:\n",
    "                s = root + line - \"Allow: \"\n",
    "                if s in link: get full contents of link\n",
    "            if \"Disallow\" in line: \n",
    "                s = root + line - \"Disallow: \"\n",
    "                if s in link: break\n",
    "            line = next line\n",
    "        # If we have reached end of specifications for crawlers without explicit allowance or disallowance for our\n",
    "        # url: get full contents \n",
    "        if (\"Allow\" not in line) & (\"Disallow\" not in line):\n",
    "            get full contents of link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the content of the links - Markus (Obsolete => we will do this in Search Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfunction(url): \n",
    "    hash_object = hashlib.md5(url.encode('utf-8')).hexdigest()\n",
    "    return hash_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test: Fction works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a7464bdbf7454b2ab532e1a6c882cd68'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashfunction('https://en.wikipedia.org/wiki/Chocolate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a7464bdbf7454b2ab532e1a6c882cd68\n"
     ]
    }
   ],
   "source": [
    "hash_object = hashlib.md5(b'https://en.wikipedia.org/wiki/Chocolate')\n",
    "print(hash_object.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_website_locally (url): \n",
    "    response = requests.get(url, allow_redirects=True)\n",
    "    filename = hashfunction(url)\n",
    "    open('data/' + filename + '.txt', 'wb').write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/a7464bdbf7454b2ab532e1a6c882cd68.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1b158b126da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstore_website_locally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://en.wikipedia.org/wiki/Chocolate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-8bad30c140f7>\u001b[0m in \u001b[0;36mstore_website_locally\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/a7464bdbf7454b2ab532e1a6c882cd68.txt'"
     ]
    }
   ],
   "source": [
    "store_website_locally('https://en.wikipedia.org/wiki/Chocolate')"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
