{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse as parse\n",
    "import urllib.robotparser as rp\n",
    "\n",
    "from socket import gethostbyname, gaierror\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 50     # max nb of links to crawl\n",
    "max_cont = 20      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one function to retrieve all external links in the page (links to the same domain do not begin by htts, and as such are not spotted by Beautiful Soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    counter = len(Links)\n",
    "        \n",
    "    Links, counter = get_all_links (url, counter, Links, max_l)\n",
    "    #html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    #soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "      \n",
    "\n",
    "    #for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    #    if(counter < max_l)&(link not in Links):\n",
    "    #        Links.append(link.get('href'))\n",
    "    #        counter += 1\n",
    "    #    else:\n",
    "    #        break\n",
    "            \n",
    "    # If we stll do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.pnas.org/content/108/21/8595', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link (url, filename):\n",
    "    doc = requests.get(url)\n",
    "    # Put here the path where you want to save the link's contents\n",
    "    name = \"Docs_crawled4/\" + filename + \".html\"\n",
    "    with open(name, 'wb') as fOut:\n",
    "        fOut.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convention: the filename will be the url's index in the list, on 4 characters\n",
    "\n",
    "# Prepare new filename\n",
    "def new_filename (count, fname = '0000', conv = [3, 1]):\n",
    "    nb_zeros = conv[0]\n",
    "    pw = conv[1]\n",
    "    \n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count)\n",
    "    \n",
    "    return (filename, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "\n",
    "conventions = [nb_zeros, pw]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    filename, conventions = new_filename(count, filename, conventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.wikipedia.org\n",
      "https://en.wikipedia.org/robots.txt\n"
     ]
    }
   ],
   "source": [
    "# Test on URL = wikipedia\n",
    "\n",
    "# Getting domain url and robots.txt link\n",
    "## Using urlparse\n",
    "URL_parsed = parse.urlparse(URL)\n",
    "print(URL_parsed.netloc)\n",
    "URL_parsed\n",
    "\n",
    "robot = URL_parsed.scheme + \"://\" + URL_parsed.netloc + \"/robots.txt\"\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/robots.txt\n",
      "http://www.pnas.org/content/108/21/robots.txt\n"
     ]
    }
   ],
   "source": [
    "## Using urljoin -DOESN'T WORK \n",
    "robot = parse.urljoin(URL, \"robots.txt\")\n",
    "print(robot)\n",
    "\n",
    "robot = parse.urljoin(links_crawled[2], \"robots.txt\")\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robot parsing\n",
    "\n",
    "RP = rp.RobotFileParser()            # Define Robot parser object\n",
    "\n",
    "#robot = \"https://en.wikipedia.org/robots.txt\"\n",
    "RP.set_url(robot)                    # Set robots.txt link\n",
    "\n",
    "RP.read()                            # Open robots.txt file\n",
    "rrate = RP.request_rate(\"*\")         # Return requests per second rate (requests, seconds)\n",
    "print(RP.crawl_delay(\"*\"))           # Returns crawl delay\n",
    "\n",
    "RP.can_fetch(\"*\", URL)               # Returns bool if can crawl that url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if robots.txt exists -> doesn't work for our second link\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            print(1)\n",
    "            return True\n",
    "        else:\n",
    "            print(0)\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        # handle your exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function bringing it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode: Crawler (nb_max_url, [URL], Domains, Links):\n",
    "\n",
    "Get robots.txt url + domain \n",
    "Save domain, robots.txt url and crawl delay if don't already have it\n",
    "\n",
    "If can fetch: \n",
    "\n",
    "    If domain in domains & time crawled > 0:\n",
    "        Wait max(0, crawl delay - (current time - time crawled))\n",
    "    \n",
    "    Crawl: save contents \n",
    "    Update time crawled\n",
    "    Save links if we still need more\n",
    "    Save in matrix domains line: domain, crawl delay (20ms if none), time crawled\n",
    "\n",
    "If haven't crawled enough links: repeat\n",
    "    \n",
    "Links = list of links we can crawl\n",
    "Domains: [domain, robots.txt link, crawl delay, time last crawled]\n",
    "(several links can have same domain)\n",
    "\n",
    "Auxiliary functions: extract up to M links from a page, new filename, save a link (all checked individually, work)\n",
    "\n",
    "To be able to wait for crawler delay time, need functions get current time, wait a certain time (need to know units of said time!!!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function with Domain matrix as list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "robots = []\n",
    "delays = []\n",
    "time_crawled = []\n",
    "\n",
    "Links = [URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with D as list of lists (domains, robots, delay, time_crawled)\n",
    "\n",
    "def Crawler_list (D = [domains, robots, delays, time_crawled], L=Links, pointer=0, fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    print(\"Nb links in Links: \", counter)\n",
    "    print(\"Pointer: \", pointer)\n",
    "    already_crawled = True\n",
    "    \n",
    "    # Issue with some links: don't parse them        \n",
    "    #if not (pointer in [1, 4]):\n",
    "    \n",
    "    url = L[pointer]\n",
    "\n",
    "    # Get url domain\n",
    "    URL_parsed = parse.urlparse(url)\n",
    "    dom = URL_parsed.netloc\n",
    "    print(\"Domain: \", dom)\n",
    "\n",
    "    # If we have not already tried to crawl in the domain\n",
    "    if not (dom in domains):\n",
    "\n",
    "        print(\"New domain\")\n",
    "        already_crawled = False        \n",
    "        # Get robots.txt link\n",
    "        r = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "        #print(\"Robots.txt link: \", r)\n",
    "\n",
    "        domains.append(dom)\n",
    "        robots.append(r)\n",
    "        delays.append(20)\n",
    "        time_crawled.append(-1)\n",
    "\n",
    "\n",
    "    i = domains.index(dom)\n",
    "    print(\"Domain index: \", i)\n",
    "    r = robots[i]\n",
    "    print(\"Robots.txt link: \", r)\n",
    "\n",
    "    \n",
    "    if (not dom in [\"www.cfsan.fda.gov\", \"www.cocoatree.org\"]):\n",
    "        \n",
    "        # Access robots.txt\n",
    "        RP = rp.RobotFileParser()         \n",
    "        RP.set_url(r) \n",
    "        RP.read()\n",
    "\n",
    "         # If the robot parser correctly opens the robots.txt\n",
    "        if (len(RP.entries) != 0):\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if ((not already_crawled) & (RP.crawl_delay(\"*\") != None)):\n",
    "                delays.pop(i) \n",
    "                delays.insert(i, RP.crawl_delay(\"*\"))\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", url)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                #if (already_crawled):\n",
    "                    #wait = max(0, delays[i] - (time.time() - time_crawled)[i])\n",
    "                    #if (wait > 0): time.sleep(wait)\n",
    "\n",
    "                # Save contents of the link\n",
    "                save_link(url, fname)\n",
    "                max_c -= 1\n",
    "                print(\"We are saving the contens of \", url)\n",
    "\n",
    "                # Update time we last crawled the domain\n",
    "                # time_crawled.pop(i)\n",
    "                # time_crawled = time.time()                  \n",
    "\n",
    "                # Prepare filename for next page\n",
    "                fname, conv = new_filename(pointer +1, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referenced in the page\n",
    "                if (len(L) < max_l) | (pointer == len(L) - 1):\n",
    "                    L, counter = get_all_links(url, counter, L, max_l)\n",
    "        \n",
    "    # Whatever happens, we move on to the next file\n",
    "    pointer += 1\n",
    "    print(D)\n",
    "    #print(L)\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (max_c > 0):\n",
    "        D, L = Crawler_list (D, L, pointer, fname, max_l, max_c, conv)\n",
    "    \n",
    "    return (D, L)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb links in Links:  1\n",
      "Pointer:  0\n",
      "Domain:  en.wikipedia.org\n",
      "New domain\n",
      "Domain index:  0\n",
      "Robots.txt link:  https://en.wikipedia.org/robots.txt\n",
      "We are saving the contens of  https://en.wikipedia.org/wiki/Chocolate\n",
      "[['en.wikipedia.org'], ['https://en.wikipedia.org/robots.txt'], [20], [-1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  1\n",
      "Domain:  ndb.nal.usda.gov\n",
      "New domain\n",
      "Domain index:  1\n",
      "Robots.txt link:  http://ndb.nal.usda.gov/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt'], [20, 20], [-1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  2\n",
      "Domain:  www.eluniversal.com.mx\n",
      "New domain\n",
      "Domain index:  2\n",
      "Robots.txt link:  http://www.eluniversal.com.mx/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt'], [20, 20, 20], [-1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  3\n",
      "Domain:  archive.fieldmuseum.org\n",
      "New domain\n",
      "Domain index:  3\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt'], [20, 20, 20, 20], [-1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  4\n",
      "Domain:  www.bartleby.com\n",
      "New domain\n",
      "Domain index:  4\n",
      "Robots.txt link:  http://www.bartleby.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt'], [20, 20, 20, 20, 20], [-1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  5\n",
      "Domain:  antiquity.ac.uk\n",
      "New domain\n",
      "Domain index:  5\n",
      "Robots.txt link:  http://antiquity.ac.uk/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt'], [20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  6\n",
      "Domain:  news.sciencemag.org\n",
      "New domain\n",
      "Domain index:  6\n",
      "Robots.txt link:  http://news.sciencemag.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  7\n",
      "Domain:  www.museum.upenn.edu\n",
      "New domain\n",
      "Domain index:  7\n",
      "Robots.txt link:  http://www.museum.upenn.edu/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  8\n",
      "Domain:  archive.fieldmuseum.org\n",
      "Domain index:  3\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  9\n",
      "Domain:  archive.fieldmuseum.org\n",
      "Domain index:  3\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  10\n",
      "Domain:  archive.fieldmuseum.org\n",
      "Domain index:  3\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  11\n",
      "Domain:  archive.fieldmuseum.org\n",
      "Domain index:  3\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  12\n",
      "Domain:  findarticles.com\n",
      "New domain\n",
      "Domain index:  8\n",
      "Robots.txt link:  http://findarticles.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  13\n",
      "Domain:  www.newyorker.com\n",
      "New domain\n",
      "Domain index:  9\n",
      "Robots.txt link:  http://www.newyorker.com/robots.txt\n",
      "We are saving the contens of  http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  14\n",
      "Domain:  www.exploratorium.edu\n",
      "New domain\n",
      "Domain index:  10\n",
      "Robots.txt link:  http://www.exploratorium.edu/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  15\n",
      "Domain:  www.smithsonianmag.com\n",
      "New domain\n",
      "Domain index:  11\n",
      "Robots.txt link:  http://www.smithsonianmag.com/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are saving the contens of  http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  16\n",
      "Domain:  hdl.handle.net\n",
      "New domain\n",
      "Domain index:  12\n",
      "Robots.txt link:  http://hdl.handle.net/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  17\n",
      "Domain:  www.history.com\n",
      "New domain\n",
      "Domain index:  13\n",
      "Robots.txt link:  http://www.history.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  18\n",
      "Domain:  www.bartleby.com\n",
      "Domain index:  4\n",
      "Robots.txt link:  http://www.bartleby.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  19\n",
      "Domain:  www.etymonline.com\n",
      "New domain\n",
      "Domain index:  14\n",
      "Robots.txt link:  http://www.etymonline.com/robots.txt\n",
      "We are saving the contens of  http://www.etymonline.com/?term=Chocolate\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  20\n",
      "Domain:  eur-lex.europa.eu\n",
      "New domain\n",
      "Domain index:  15\n",
      "Robots.txt link:  http://eur-lex.europa.eu/robots.txt\n",
      "We are saving the contens of  http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  21\n",
      "Domain:  www.cfsan.fda.gov\n",
      "New domain\n",
      "Domain index:  16\n",
      "Robots.txt link:  http://www.cfsan.fda.gov/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  22\n",
      "Domain:  www.worldcocoafoundation.org\n",
      "New domain\n",
      "Domain index:  17\n",
      "Robots.txt link:  http://www.worldcocoafoundation.org/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  23\n",
      "Domain:  www.spiritofaloha.com\n",
      "New domain\n",
      "Domain index:  18\n",
      "Robots.txt link:  http://www.spiritofaloha.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  24\n",
      "Domain:  news.bbc.co.uk\n",
      "New domain\n",
      "Domain index:  19\n",
      "Robots.txt link:  http://news.bbc.co.uk/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  25\n",
      "Domain:  thecnnfreedomproject.blogs.cnn.com\n",
      "New domain\n",
      "Domain index:  20\n",
      "Robots.txt link:  http://thecnnfreedomproject.blogs.cnn.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  26\n",
      "Domain:  www.pantagraph.com\n",
      "New domain\n",
      "Domain index:  21\n",
      "Robots.txt link:  http://www.pantagraph.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  27\n",
      "Domain:  www.bbc.co.uk\n",
      "New domain\n",
      "Domain index:  22\n",
      "Robots.txt link:  http://www.bbc.co.uk/robots.txt\n",
      "We are saving the contens of  http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  28\n",
      "Domain:  worldcocoafoundation.org\n",
      "New domain\n",
      "Domain index:  23\n",
      "Robots.txt link:  http://worldcocoafoundation.org/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  29\n",
      "Domain:  www.dallasfood.org\n",
      "New domain\n",
      "Domain index:  24\n",
      "Robots.txt link:  http://www.dallasfood.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  30\n",
      "Domain:  www.businessinsider.com\n",
      "New domain\n",
      "Domain index:  25\n",
      "Robots.txt link:  http://www.businessinsider.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  31\n",
      "Domain:  www.botanical.com\n",
      "New domain\n",
      "Domain index:  26\n",
      "Robots.txt link:  http://www.botanical.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  32\n",
      "Domain:  www.xocoatl.org\n",
      "New domain\n",
      "Domain index:  27\n",
      "Robots.txt link:  http://www.xocoatl.org/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  33\n",
      "Domain:  www.ama.ab.ca\n",
      "New domain\n",
      "Domain index:  28\n",
      "Robots.txt link:  http://www.ama.ab.ca/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org', 'www.ama.ab.ca'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt', 'http://www.ama.ab.ca/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  34\n",
      "Domain:  www.metaefficient.com\n",
      "New domain\n",
      "Domain index:  29\n",
      "Robots.txt link:  http://www.metaefficient.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org', 'www.ama.ab.ca', 'www.metaefficient.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt', 'http://www.ama.ab.ca/robots.txt', 'http://www.metaefficient.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  35\n",
      "Domain:  www.sensationalchocolates.com\n",
      "New domain\n",
      "Domain index:  30\n",
      "Robots.txt link:  http://www.sensationalchocolates.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org', 'www.ama.ab.ca', 'www.metaefficient.com', 'www.sensationalchocolates.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt', 'http://www.ama.ab.ca/robots.txt', 'http://www.metaefficient.com/robots.txt', 'http://www.sensationalchocolates.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  36\n",
      "Domain:  www.icco.org\n",
      "New domain\n",
      "Domain index:  31\n",
      "Robots.txt link:  http://www.icco.org/robots.txt\n",
      "We are saving the contens of  http://www.icco.org/questions/varieties.htm\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org', 'www.ama.ab.ca', 'www.metaefficient.com', 'www.sensationalchocolates.com', 'www.icco.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt', 'http://www.ama.ab.ca/robots.txt', 'http://www.metaefficient.com/robots.txt', 'http://www.sensationalchocolates.com/robots.txt', 'http://www.icco.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  37\n",
      "Domain:  www.xocoatl.org\n",
      "Domain index:  27\n",
      "Robots.txt link:  http://www.xocoatl.org/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org', 'www.ama.ab.ca', 'www.metaefficient.com', 'www.sensationalchocolates.com', 'www.icco.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt', 'http://www.ama.ab.ca/robots.txt', 'http://www.metaefficient.com/robots.txt', 'http://www.sensationalchocolates.com/robots.txt', 'http://www.icco.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  38\n",
      "Domain:  api.ning.com\n",
      "New domain\n",
      "Domain index:  32\n",
      "Robots.txt link:  http://api.ning.com/robots.txt\n",
      "[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com', 'www.exploratorium.edu', 'www.smithsonianmag.com', 'hdl.handle.net', 'www.history.com', 'www.etymonline.com', 'eur-lex.europa.eu', 'www.cfsan.fda.gov', 'www.worldcocoafoundation.org', 'www.spiritofaloha.com', 'news.bbc.co.uk', 'thecnnfreedomproject.blogs.cnn.com', 'www.pantagraph.com', 'www.bbc.co.uk', 'worldcocoafoundation.org', 'www.dallasfood.org', 'www.businessinsider.com', 'www.botanical.com', 'www.xocoatl.org', 'www.ama.ab.ca', 'www.metaefficient.com', 'www.sensationalchocolates.com', 'www.icco.org', 'api.ning.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt', 'http://www.exploratorium.edu/robots.txt', 'http://www.smithsonianmag.com/robots.txt', 'http://hdl.handle.net/robots.txt', 'http://www.history.com/robots.txt', 'http://www.etymonline.com/robots.txt', 'http://eur-lex.europa.eu/robots.txt', 'http://www.cfsan.fda.gov/robots.txt', 'http://www.worldcocoafoundation.org/robots.txt', 'http://www.spiritofaloha.com/robots.txt', 'http://news.bbc.co.uk/robots.txt', 'http://thecnnfreedomproject.blogs.cnn.com/robots.txt', 'http://www.pantagraph.com/robots.txt', 'http://www.bbc.co.uk/robots.txt', 'http://worldcocoafoundation.org/robots.txt', 'http://www.dallasfood.org/robots.txt', 'http://www.businessinsider.com/robots.txt', 'http://www.botanical.com/robots.txt', 'http://www.xocoatl.org/robots.txt', 'http://www.ama.ab.ca/robots.txt', 'http://www.metaefficient.com/robots.txt', 'http://www.sensationalchocolates.com/robots.txt', 'http://www.icco.org/robots.txt', 'http://api.ning.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 10, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\n",
      "Nb links in Links:  50\n",
      "Pointer:  39\n",
      "Domain:  www.cocoatree.org\n",
      "New domain\n",
      "Domain index:  33\n",
      "Robots.txt link:  http://www.cocoatree.org/robots.txt\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -2] Name or service not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1318\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 946\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    947\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-5c5eecfe65d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mLinks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# RecursionError: maximum recursion depth exceeded while calling a Python object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_c\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-71-a1aaa4313bc8>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, L, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mRP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRobotFileParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mRP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mRP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m          \u001b[0;31m# If the robot parser correctly opens the robots.txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/robotparser.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"\"\"Reads the robots.txt URL and feeds it to the parser.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0mhttp_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -2] Name or service not known>"
     ]
    }
   ],
   "source": [
    "# Emptying lists from previous tests\n",
    "for l in [domains, robots, delays, time_crawled]:\n",
    "    while len(l) > 0:\n",
    "            l.pop(0)\n",
    "\n",
    "while (len(Links) > 1):\n",
    "    Links.pop(1)\n",
    "\n",
    "D, Links = Crawler_list()\n",
    "\n",
    "# RecursionError: maximum recursion depth exceeded while calling a Python object\n",
    "# Parser error: should be able to crawl links 2 & 3 but doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n"
     ]
    }
   ],
   "source": [
    "print(Links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 5], ['aba', 'baa', 'daa', 'ett'], [20, 20, 20, 50], [1, 2.3, 2.4, 3]]\n",
      "['aba', 'gg', 'daa', 'ett']\n",
      "[1, 4, 2.4, 3]\n",
      "[[1, 2, 3, 5], ['aba', 'gg', 'daa', 'ett'], [20, 20, 20, 50], [1, 4, 2.4, 3]]\n"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains: [domains, robot_links, delay, time]\n",
    "\n",
    "domains = [1, 2, 3, 5]\n",
    "robot_links = [\"aba\", \"baa\", \"daa\", \"ett\"]\n",
    "delay = [20, 20, 20, 50]\n",
    "time = [1, 2.3, 2.4, 3]\n",
    "\n",
    "D = [domains, robot_links, delay, time]\n",
    "print(D)\n",
    "\n",
    "dom = 2\n",
    "\n",
    "# pop then insert method done directly in list and D\n",
    "if (dom in domains):\n",
    "    i = domains.index(dom)\n",
    "    \n",
    "    #robot_links[i] = \"g\"\n",
    "    robot_links.pop(i)\n",
    "    robot_links.insert(i, \"gg\")\n",
    "    print(robot_links)\n",
    "    \n",
    "    time.pop(i)\n",
    "    time.insert(i, 4)\n",
    "    print(time)\n",
    "    \n",
    "# append done directly in domains and D\n",
    "else:\n",
    "    i = len(domains)\n",
    "    domains.append(dom)\n",
    "    robot_links.append(\"c\")\n",
    "    delay.append(20)\n",
    "    time.append(-1)\n",
    "\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-ea69e73f2215>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-ea69e73f2215>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    D2 = D[][3]\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains: [domain 1, domain 2,...] -> Obsolete\n",
    "\n",
    "D = [[1, \"a\", 20, 1], [2, \"b\", 20, 2.3], [5, \"d\", 30, 2.4]]\n",
    "\n",
    "#i= None\n",
    "j = 0\n",
    "#while (not i):\n",
    "#    i = D[j].index(5)\n",
    "#    j += 1\n",
    "\n",
    "while (5 not in D[j]):\n",
    "    j += 1\n",
    "\n",
    "print(j)\n",
    "\n",
    "D2 = D[][3]\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function with Domains matrix as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with Domains as a pandas dataframe\n",
    "\n",
    "def Crawler (Domains = pd.DataFrame({\"domains\": [],\"robot_links\": [],\"crawl_delay\": [],\"time\": []}), Links=[URL], pointer=0, fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "          \n",
    "    url = Links[pointer]\n",
    "    already_crawled = True\n",
    "    counter = len(Links)\n",
    "    \n",
    "    if (pointer != 1):\n",
    "    \n",
    "        # Get url domain\n",
    "        URL_parsed = parse.urlparse(url)\n",
    "        dom = URL_parsed.netloc\n",
    "\n",
    "        # If we have not already tried to crawl in the domain\n",
    "        if (not dom in Domains[\"domains\"]):\n",
    "            already_crawled = False\n",
    "            # Get robots.txt link\n",
    "            robot = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "\n",
    "            D = pd.DataFrame ({\n",
    "                \"domains\": [dom],\n",
    "                \"robot_links\": [robot],\n",
    "                \"crawl_delay\": [20],\n",
    "                \"time\": [-1]\n",
    "            })        \n",
    "            Domains = pd.concat([Domains, D])\n",
    "            Domains.reset_index(drop=True, inplace=True)\n",
    "            print(Domains.head())\n",
    "\n",
    "        # Set i the index of the row containing our current domain\n",
    "        i = Domains.index[Domains[\"domains\"] == dom].tolist()[0]\n",
    "\n",
    "        robot = Domains.loc[i, \"robot_links\"]\n",
    "        print(robot)\n",
    "\n",
    "        # If robots.txt exists:\n",
    "        if (does_url_exist(robot)):\n",
    "\n",
    "            # Access robots.txt\n",
    "            RP = rp.RobotFileParser()         \n",
    "            RP.set_url(robot)  \n",
    "            RP.read()\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if (not already_crawled) & (RP.crawl_delay(\"*\") != None):\n",
    "                Domains.loc[i, \"crawl_delay\"] = RP.crawl_delay(\"*\")\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", URL)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                #if (already_crawled):\n",
    "                #    Wait max(0, crawl delay - (current time - time crawled))\n",
    "                # Use time.sleep(time to wait), first import time\n",
    "                # time.time() = get_cpu time now\n",
    "\n",
    "                # Save contents of the link\n",
    "                save_link(url, fname)\n",
    "                # Domains.loc[i, \"time\"] = current_time                  # update time we last crawled the domain\n",
    "\n",
    "                # Prepare filename for next page\n",
    "                pointer += 1\n",
    "                fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referencec in the page\n",
    "                if len(Links) < max_l:\n",
    "                    Links, counter = get_all_links(url, counter, Links, max_l)\n",
    "\n",
    "                print(pointer)\n",
    "                \n",
    "    else: \n",
    "        pointer +=1\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (pointer < max_c):\n",
    "        Domains, Links = Crawler (Domains, Links, pointer, fname, max(0, max_l - counter), (max_c - pointer), conv)\n",
    "        \n",
    "    return (Domains, Links)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            domains                          robot_links  crawl_delay  time\n",
      "0  en.wikipedia.org  https://en.wikipedia.org/robots.txt         20.0  -1.0\n",
      "https://en.wikipedia.org/robots.txt\n",
      "1\n",
      "1\n",
      "            domains                          robot_links  crawl_delay  time\n",
      "0  en.wikipedia.org  https://en.wikipedia.org/robots.txt         20.0  -1.0\n",
      "1      www.pnas.org       http://www.pnas.org/robots.txt         20.0  -1.0\n",
      "http://www.pnas.org/robots.txt\n",
      "1\n",
      "3\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                  domains                               robot_links  \\\n",
       " 0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
       " 1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
       " 2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 5  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 6  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " \n",
       "    crawl_delay  time  \n",
       " 0         20.0  -1.0  \n",
       " 1         10.0  -1.0  \n",
       " 2         20.0  -1.0  \n",
       " 3         20.0  -1.0  \n",
       " 4         20.0  -1.0  \n",
       " 5         20.0  -1.0  \n",
       " 6         20.0  -1.0  ,\n",
       " ['https://en.wikipedia.org/wiki/Chocolate',\n",
       "  'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate',\n",
       "  'http://www.pnas.org/content/108/21/8595',\n",
       "  'http://www.eluniversal.com.mx/notas/526113.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html',\n",
       "  'http://www.bartleby.com/61/68/C0316800.html',\n",
       "  'http://antiquity.ac.uk/projgall/powis/index.html',\n",
       "  'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america',\n",
       "  'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html',\n",
       "  'http://archive.fieldmuseum.org/chocolate/history.html',\n",
       "  'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999',\n",
       "  'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford',\n",
       "  'http://www.exploratorium.edu/exploring/exploring_chocolate/',\n",
       "  'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist',\n",
       "  'http://hdl.handle.net/2027/spo.did2222.0000.474',\n",
       "  'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate',\n",
       "  'http://www.bartleby.com/61/68/C0316800.html',\n",
       "  'http://www.etymonline.com/?term=Chocolate',\n",
       "  'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT',\n",
       "  'http://www.cfsan.fda.gov/~lrd/fr021004.html',\n",
       "  'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf',\n",
       "  'http://www.spiritofaloha.com/features/0907/rarest.html',\n",
       "  'http://news.bbc.co.uk/2/hi/africa/2042474.stm',\n",
       "  'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/',\n",
       "  'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt',\n",
       "  'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml',\n",
       "  'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf',\n",
       "  'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78',\n",
       "  'http://www.businessinsider.com/when-chocolate-extinct-2017-12',\n",
       "  'http://www.botanical.com/botanical/mgmh/c/cacao-02.html',\n",
       "  'http://www.xocoatl.org/tree.htm',\n",
       "  'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm',\n",
       "  'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html',\n",
       "  'http://www.sensationalchocolates.com/ingredients.html',\n",
       "  'http://www.icco.org/questions/varieties.htm',\n",
       "  'http://www.xocoatl.org/harvest.htm',\n",
       "  'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf',\n",
       "  'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp',\n",
       "  'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf',\n",
       "  'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf',\n",
       "  'http://news.bbc.co.uk/1/hi/uk/678141.stm',\n",
       "  'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66',\n",
       "  'http://www.sallys-place.com/food/columns/zonis/conching.htm',\n",
       "  'http://candy.about.com/od/candybasics/ht/temperchoc.htm',\n",
       "  'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf',\n",
       "  'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf',\n",
       "  'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   domains robots_link  crawl_delay  time\n",
      "0        1           a           20   1.0\n",
      "1        2           b           20   2.3\n",
      "2        3           d           20   2.4\n",
      "3        5           e           50   3.0\n",
      "2    d\n",
      "Name: robots_link, dtype: object\n",
      "2    d\n",
      "Name: robots_link, dtype: object\n",
      "2\n",
      "i\n",
      "   domains robots_link  crawl_delay  time\n",
      "0        1           a           20   1.0\n",
      "1        2           b           20   2.3\n",
      "2        3           i           20   2.4\n",
      "3        5           e           50   3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clothilde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-8d6cb89889c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"domains\"\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "# Test dataframe methods on Domains \n",
    "\n",
    "D = pd.DataFrame ({\n",
    "    \"domains\": [1, 2, 3, 5],\n",
    "    \"robots_link\": [\"a\", \"b\", \"d\", \"e\"],\n",
    "    \"crawl_delay\": [20, 20, 20, 50],\n",
    "    \"time\": [1, 2.3, 2.4, 3]\n",
    "})\n",
    "\n",
    "print(D.head())\n",
    "\n",
    "#if 10 not in D[\"domains\"]:\n",
    "#    pd.melt(D, [10, \"g\", 20, 3.2]) #=> Melt only with df \n",
    "#print(D.head())\n",
    "\n",
    "dom = 3\n",
    "\n",
    "curr = D.loc[D[\"domains\"] == 3]   # Makes copy of row!!! Can't modif that\n",
    "\n",
    "#i = D[D[\"domains\"] == 3].index\n",
    "#print(i)\n",
    "#D[i, 2] = 10\n",
    "#print(D.head())\n",
    "\n",
    "#r = D[D[\"domains\"] == dom]\n",
    "#print(r)\n",
    "#print(r[\"time\"])\n",
    "\n",
    "r = D[D[\"domains\"] == dom][\"robots_link\"]\n",
    "print(r)\n",
    "\n",
    "D[D[\"domains\"] == dom][\"robots_link\"] = \"h\"\n",
    "print(D[D[\"domains\"] == dom][\"robots_link\"])\n",
    "\n",
    "i = D.index[D[\"domains\"] == dom].tolist()[0]\n",
    "print(i)\n",
    "\n",
    "D.loc[i, \"robots_link\"] = \"i\"\n",
    "print(D.loc[i, \"robots_link\"])\n",
    "print(D.head())\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
