{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse as parse\n",
    "import urllib.robotparser as rp\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 50     # max nb of links to crawl\n",
    "max_cont = 20      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    counter = len(Links)\n",
    "        \n",
    "    Links, counter = get_all_links (url, counter, Links, max_l)\n",
    "    #html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    #soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "      \n",
    "\n",
    "    #for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    #    if(counter < max_l)&(link not in Links):\n",
    "    #        Links.append(link.get('href'))\n",
    "    #        counter += 1\n",
    "    #    else:\n",
    "    #        break\n",
    "            \n",
    "    # If we stll do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.pnas.org/content/108/21/8595', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link (url, filename):\n",
    "    doc = requests.get(url)\n",
    "    # Put here the path where you want to save the link's contents\n",
    "    name = \"Docs_crawled4/\" + filename + \".html\"\n",
    "    with open(name, 'wb') as fOut:\n",
    "        fOut.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convention: the filename will be the url's index in the list, on 4 characters\n",
    "\n",
    "# Prepare new filename\n",
    "def new_filename (count, fname = '0000', conv = [3, 1]):\n",
    "    nb_zeros = conv[0]\n",
    "    pw = conv[1]\n",
    "    \n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count)\n",
    "    \n",
    "    return (filename, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "\n",
    "conventions = [nb_zeros, pw]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    filename, conventions = new_filename(count, filename, conventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.wikipedia.org\n",
      "https://en.wikipedia.org/robots.txt\n"
     ]
    }
   ],
   "source": [
    "# Test on URL = wikipedia\n",
    "\n",
    "# Getting domain url and robots.txt link\n",
    "## Using urlparse\n",
    "URL_parsed = parse.urlparse(URL)\n",
    "print(URL_parsed.netloc)\n",
    "URL_parsed\n",
    "\n",
    "robot = URL_parsed.scheme + \"://\" + URL_parsed.netloc + \"/robots.txt\"\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/robots.txt\n",
      "http://www.pnas.org/content/108/21/robots.txt\n"
     ]
    }
   ],
   "source": [
    "## Using urljoin -DOESN'T WORK \n",
    "robot = parse.urljoin(URL, \"robots.txt\")\n",
    "print(robot)\n",
    "\n",
    "robot = parse.urljoin(links_crawled[2], \"robots.txt\")\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robot parsing\n",
    "\n",
    "RP = rp.RobotFileParser()            # Define Robot parser object\n",
    "\n",
    "#robot = \"https://en.wikipedia.org/robots.txt\"\n",
    "RP.set_url(robot)                    # Set robots.txt link\n",
    "\n",
    "RP.read()                            # Open robots.txt file\n",
    "rrate = RP.request_rate(\"*\")         # Return requests per second rate (requests, seconds)\n",
    "print(RP.crawl_delay(\"*\"))           # Returns crawl delay\n",
    "\n",
    "RP.can_fetch(\"*\", URL)               # Returns bool if can crawl that url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if robots.txt exists -> doesn't work for our second link\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            print(1)\n",
    "            return True\n",
    "        else:\n",
    "            print(0)\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        # handle your exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function bringing it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode: Crawler (nb_max_url, [URL], Domains, Links):\n",
    "\n",
    "Get robots.txt url + domain \n",
    "Save domain, robots.txt url and crawl delay if don't already have it\n",
    "\n",
    "If can fetch: \n",
    "\n",
    "    If domain in domains & time crawled > 0:\n",
    "        Wait max(0, crawl delay - (current time - time crawled))\n",
    "    \n",
    "    Crawl: save contents \n",
    "    Update time crawled\n",
    "    Save links if we still need more\n",
    "    Save in matrix domains line: domain, crawl delay (20ms if none), time crawled\n",
    "\n",
    "If haven't crawled enough links: repeat\n",
    "    \n",
    "Links = list of links we can crawl\n",
    "Domains: [domain, robots.txt link, crawl delay, time last crawled]\n",
    "(several links can have same domain)\n",
    "\n",
    "Auxiliary functions: extract up to M links from a page, new filename, save a link (all checked individually, work)\n",
    "\n",
    "To be able to wait for crawler delay time, need functions get current time, wait a certain time (need to know units of said time!!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "robots = []\n",
    "delays = []\n",
    "time_crawled = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with D as list of lists (domains, robots, delay, time_crawled)\n",
    "\n",
    "def Crawler_list (D = [domains, robots, delays, time_crawled], Links=[URL], pointer=0, fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(Links)\n",
    "    print(\"Nb links in Links: \", counter)\n",
    "    print(\"Pointer: \", pointer)\n",
    "    \n",
    "    # Issue with 2nd link \n",
    "    if (pointer == 1):\n",
    "        pointer +=1\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        url = Links[pointer]\n",
    "        already_crawled = True\n",
    "        \n",
    "        # Get url domain\n",
    "        URL_parsed = parse.urlparse(url)\n",
    "        dom = URL_parsed.netloc\n",
    "        print(\"Domain: \", dom)\n",
    "\n",
    "        # If we have not already tried to crawl in the domain\n",
    "        if not (dom in domains):\n",
    "            \n",
    "            print(\"New_domain\")\n",
    "            already_crawled = False        \n",
    "            # Get robots.txt link\n",
    "            r = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "            print(\"Robots.txt link: \", r)\n",
    "\n",
    "            domains.append(dom)\n",
    "            robots.append(r)\n",
    "            delays.append(20)\n",
    "            time_crawled.append(-1)\n",
    "\n",
    "\n",
    "        i = domains.index(dom)\n",
    "        print(\"Domain index: \", i)\n",
    "        r = robots[i]\n",
    "        print(\"Robots.txt link: \", r)\n",
    "\n",
    "        # Access robots.txt\n",
    "        RP = rp.RobotFileParser()         \n",
    "        RP.set_url(r)                    \n",
    "        RP.read()\n",
    "\n",
    "        # Get crawl delay (if it is given and we don't already have it)\n",
    "        if ((not already_crawled) & (RP.crawl_delay(\"*\") != None)):\n",
    "            robots.pop(i) \n",
    "            robots.insert(i, RP.crawl_delay(\"*\"))\n",
    "\n",
    "        # If we can crawl the file\n",
    "        if (RP.can_fetch(\"*\", URL)):\n",
    "\n",
    "            # Wait appropriate time if we have already crawled the domain\n",
    "            #if (already_crawled):\n",
    "                #wait = max(0, crawl delay - (current time - time crawled))\n",
    "                #if (wait >0): time.sleep(wait)\n",
    "\n",
    "            # Save contents of the link\n",
    "            save_link(url, fname)\n",
    "            print(\"We are saving a link from domain \", dom)\n",
    "            # time_crawled.pop(i)\n",
    "            # time_crawled = time.time()                  # update time we last crawled the domain\n",
    "\n",
    "            # Prepare filename for next page\n",
    "            pointer += 1\n",
    "            fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "            # If we do not have enough links, save those referenced in the page\n",
    "            if len(Links) < max_l:\n",
    "                Links, counter = get_all_links(url, counter, Links, max_l)\n",
    "                \n",
    "        # If we can't crawl the file, we still move on to the next one\n",
    "        else:\n",
    "            pointer += 1\n",
    "    \n",
    "    print(D)\n",
    "    print(Links)\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (pointer < max_c):\n",
    "        D2, Links2 = Crawler_list (D, Links, pointer, fname, max(0, max_l - counter), max_c, conv)\n",
    "    \n",
    "    return (D2, Links2)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb links in Links:  1\n",
      "Pointer:  0\n",
      "Domain:  en.wikipedia.org\n",
      "New_domain\n",
      "Robots.txt link:  https://en.wikipedia.org/robots.txt\n",
      "Domain index:  0\n",
      "Robots.txt link:  https://en.wikipedia.org/robots.txt\n",
      "We are saving a link from domain  en.wikipedia.org\n",
      "[['en.wikipedia.org'], ['https://en.wikipedia.org/robots.txt'], [20], [-1]]\n",
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n",
      "Nb links in Links:  50\n",
      "Pointer:  1\n",
      "[['en.wikipedia.org'], ['https://en.wikipedia.org/robots.txt'], [20], [-1]]\n",
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n",
      "Nb links in Links:  50\n",
      "Pointer:  2\n",
      "Domain:  www.eluniversal.com.mx\n",
      "New_domain\n",
      "Robots.txt link:  http://www.eluniversal.com.mx/robots.txt\n",
      "Domain index:  1\n",
      "Robots.txt link:  http://www.eluniversal.com.mx/robots.txt\n",
      "[['en.wikipedia.org', 'www.eluniversal.com.mx'], ['https://en.wikipedia.org/robots.txt', 'http://www.eluniversal.com.mx/robots.txt'], [20, 20], [-1, -1]]\n",
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n",
      "Nb links in Links:  50\n",
      "Pointer:  3\n",
      "Domain:  archive.fieldmuseum.org\n",
      "New_domain\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n",
      "Domain index:  2\n",
      "Robots.txt link:  http://archive.fieldmuseum.org/robots.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['en.wikipedia.org', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org'], ['https://en.wikipedia.org/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt'], [20, 20, 20], [-1, -1, -1]]\n",
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n",
      "Nb links in Links:  50\n",
      "Pointer:  4\n",
      "Domain:  www.bartleby.com\n",
      "New_domain\n",
      "Robots.txt link:  http://www.bartleby.com/robots.txt\n",
      "Domain index:  3\n",
      "Robots.txt link:  http://www.bartleby.com/robots.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'delay'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c3142843ff21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# RecursionError: maximum recursion depth exceeded while calling a Python object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ee714c9cc472>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, Links, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpointer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ee714c9cc472>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, Links, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpointer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ee714c9cc472>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, Links, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpointer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ee714c9cc472>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, Links, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# If we have not saved enough links: repeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpointer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrawler_list\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ee714c9cc472>\u001b[0m in \u001b[0;36mCrawler_list\u001b[0;34m(D, Links, pointer, fname, max_l, max_c, conv)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Get crawl delay (if it is given and we don't already have it)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0malready_crawled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_delay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mrobots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mrobots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_delay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/robotparser.py\u001b[0m in \u001b[0;36mcrawl_delay\u001b[0;34m(self, useragent)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplies_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0museragent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_entry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museragent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'delay'"
     ]
    }
   ],
   "source": [
    "D, Links = Crawler_list()\n",
    "# RecursionError: maximum recursion depth exceeded while calling a Python object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 5], ['aba', 'baa', 'daa', 'ett'], [20, 20, 20, 50], [1, 2.3, 2.4, 3]]\n",
      "['aba', 'gg', 'daa', 'ett']\n",
      "[1, 4, 2.4, 3]\n",
      "[[1, 2, 3, 5], ['aba', 'gg', 'daa', 'ett'], [20, 20, 20, 50], [1, 4, 2.4, 3]]\n"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains: [domains, robot_links, delay, time]\n",
    "\n",
    "domains = [1, 2, 3, 5]\n",
    "robot_links = [\"aba\", \"baa\", \"daa\", \"ett\"]\n",
    "delay = [20, 20, 20, 50]\n",
    "time = [1, 2.3, 2.4, 3]\n",
    "\n",
    "D = [domains, robot_links, delay, time]\n",
    "print(D)\n",
    "\n",
    "dom = 2\n",
    "\n",
    "# pop then insert method done directly in list and D\n",
    "if (dom in domains):\n",
    "    i = domains.index(dom)\n",
    "    \n",
    "    #robot_links[i] = \"g\"\n",
    "    robot_links.pop(i)\n",
    "    robot_links.insert(i, \"gg\")\n",
    "    print(robot_links)\n",
    "    \n",
    "    time.pop(i)\n",
    "    time.insert(i, 4)\n",
    "    print(time)\n",
    "    \n",
    "# append done directly in domains and D\n",
    "else:\n",
    "    i = len(domains)\n",
    "    domains.append(dom)\n",
    "    robot_links.append(\"c\")\n",
    "    delay.append(20)\n",
    "    time.append(-1)\n",
    "\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-ea69e73f2215>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-ea69e73f2215>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    D2 = D[][3]\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains: [domain 1, domain 2,...] -> Obsolete\n",
    "\n",
    "D = [[1, \"a\", 20, 1], [2, \"b\", 20, 2.3], [5, \"d\", 30, 2.4]]\n",
    "\n",
    "#i= None\n",
    "j = 0\n",
    "#while (not i):\n",
    "#    i = D[j].index(5)\n",
    "#    j += 1\n",
    "\n",
    "while (5 not in D[j]):\n",
    "    j += 1\n",
    "\n",
    "print(j)\n",
    "\n",
    "D2 = D[][3]\n",
    "print(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with Domains as a pandas dataframe\n",
    "\n",
    "def Crawler (Domains = pd.DataFrame({\"domains\": [],\"robot_links\": [],\"crawl_delay\": [],\"time\": []}), Links=[URL], pointer=0, fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "          \n",
    "    url = Links[pointer]\n",
    "    already_crawled = True\n",
    "    counter = len(Links)\n",
    "    \n",
    "    if (pointer != 1):\n",
    "    \n",
    "        # Get url domain\n",
    "        URL_parsed = parse.urlparse(url)\n",
    "        dom = URL_parsed.netloc\n",
    "\n",
    "        # If we have not already tried to crawl in the domain\n",
    "        if (not dom in Domains[\"domains\"]):\n",
    "            already_crawled = False\n",
    "            # Get robots.txt link\n",
    "            robot = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "\n",
    "            D = pd.DataFrame ({\n",
    "                \"domains\": [dom],\n",
    "                \"robot_links\": [robot],\n",
    "                \"crawl_delay\": [20],\n",
    "                \"time\": [-1]\n",
    "            })        \n",
    "            Domains = pd.concat([Domains, D])\n",
    "            Domains.reset_index(drop=True, inplace=True)\n",
    "            print(Domains.head())\n",
    "\n",
    "        # Set i the index of the row containing our current domain\n",
    "        i = Domains.index[Domains[\"domains\"] == dom].tolist()[0]\n",
    "\n",
    "        robot = Domains.loc[i, \"robot_links\"]\n",
    "        print(robot)\n",
    "\n",
    "        # If robots.txt exists:\n",
    "        if (does_url_exist(robot)):\n",
    "\n",
    "            # Access robots.txt\n",
    "            RP = rp.RobotFileParser()         \n",
    "            RP.set_url(robot)  \n",
    "            RP.read()\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if (not already_crawled) & (RP.crawl_delay(\"*\") != None):\n",
    "                Domains.loc[i, \"crawl_delay\"] = RP.crawl_delay(\"*\")\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", URL)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                #if (already_crawled):\n",
    "                #    Wait max(0, crawl delay - (current time - time crawled))\n",
    "                # Use time.sleep(time to wait), first import time\n",
    "                # time.time() = get_cpu time now\n",
    "\n",
    "                # Save contents of the link\n",
    "                save_link(url, fname)\n",
    "                # Domains.loc[i, \"time\"] = current_time                  # update time we last crawled the domain\n",
    "\n",
    "                # Prepare filename for next page\n",
    "                pointer += 1\n",
    "                fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referencec in the page\n",
    "                if len(Links) < max_l:\n",
    "                    Links, counter = get_all_links(url, counter, Links, max_l)\n",
    "\n",
    "                print(pointer)\n",
    "                \n",
    "    else: \n",
    "        pointer +=1\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (pointer < max_c):\n",
    "        Domains, Links = Crawler (Domains, Links, pointer, fname, max(0, max_l - counter), (max_c - pointer), conv)\n",
    "        \n",
    "    return (Domains, Links)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            domains                          robot_links  crawl_delay  time\n",
      "0  en.wikipedia.org  https://en.wikipedia.org/robots.txt         20.0  -1.0\n",
      "https://en.wikipedia.org/robots.txt\n",
      "1\n",
      "1\n",
      "            domains                          robot_links  crawl_delay  time\n",
      "0  en.wikipedia.org  https://en.wikipedia.org/robots.txt         20.0  -1.0\n",
      "1      www.pnas.org       http://www.pnas.org/robots.txt         20.0  -1.0\n",
      "http://www.pnas.org/robots.txt\n",
      "1\n",
      "3\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n",
      "                  domains                               robot_links  \\\n",
      "0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
      "1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
      "2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
      "\n",
      "   crawl_delay  time  \n",
      "0         20.0  -1.0  \n",
      "1         10.0  -1.0  \n",
      "2         20.0  -1.0  \n",
      "3         20.0  -1.0  \n",
      "4         20.0  -1.0  \n",
      "http://www.eluniversal.com.mx/robots.txt\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                  domains                               robot_links  \\\n",
       " 0        en.wikipedia.org       https://en.wikipedia.org/robots.txt   \n",
       " 1            www.pnas.org            http://www.pnas.org/robots.txt   \n",
       " 2  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 3  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 4  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 5  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " 6  www.eluniversal.com.mx  http://www.eluniversal.com.mx/robots.txt   \n",
       " \n",
       "    crawl_delay  time  \n",
       " 0         20.0  -1.0  \n",
       " 1         10.0  -1.0  \n",
       " 2         20.0  -1.0  \n",
       " 3         20.0  -1.0  \n",
       " 4         20.0  -1.0  \n",
       " 5         20.0  -1.0  \n",
       " 6         20.0  -1.0  ,\n",
       " ['https://en.wikipedia.org/wiki/Chocolate',\n",
       "  'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate',\n",
       "  'http://www.pnas.org/content/108/21/8595',\n",
       "  'http://www.eluniversal.com.mx/notas/526113.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html',\n",
       "  'http://www.bartleby.com/61/68/C0316800.html',\n",
       "  'http://antiquity.ac.uk/projgall/powis/index.html',\n",
       "  'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america',\n",
       "  'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html',\n",
       "  'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html',\n",
       "  'http://archive.fieldmuseum.org/chocolate/history.html',\n",
       "  'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999',\n",
       "  'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford',\n",
       "  'http://www.exploratorium.edu/exploring/exploring_chocolate/',\n",
       "  'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist',\n",
       "  'http://hdl.handle.net/2027/spo.did2222.0000.474',\n",
       "  'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate',\n",
       "  'http://www.bartleby.com/61/68/C0316800.html',\n",
       "  'http://www.etymonline.com/?term=Chocolate',\n",
       "  'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT',\n",
       "  'http://www.cfsan.fda.gov/~lrd/fr021004.html',\n",
       "  'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf',\n",
       "  'http://www.spiritofaloha.com/features/0907/rarest.html',\n",
       "  'http://news.bbc.co.uk/2/hi/africa/2042474.stm',\n",
       "  'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/',\n",
       "  'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt',\n",
       "  'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml',\n",
       "  'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf',\n",
       "  'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78',\n",
       "  'http://www.businessinsider.com/when-chocolate-extinct-2017-12',\n",
       "  'http://www.botanical.com/botanical/mgmh/c/cacao-02.html',\n",
       "  'http://www.xocoatl.org/tree.htm',\n",
       "  'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm',\n",
       "  'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html',\n",
       "  'http://www.sensationalchocolates.com/ingredients.html',\n",
       "  'http://www.icco.org/questions/varieties.htm',\n",
       "  'http://www.xocoatl.org/harvest.htm',\n",
       "  'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf',\n",
       "  'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp',\n",
       "  'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf',\n",
       "  'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf',\n",
       "  'http://news.bbc.co.uk/1/hi/uk/678141.stm',\n",
       "  'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66',\n",
       "  'http://www.sallys-place.com/food/columns/zonis/conching.htm',\n",
       "  'http://candy.about.com/od/candybasics/ht/temperchoc.htm',\n",
       "  'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf',\n",
       "  'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf',\n",
       "  'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white'])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   domains robots_link  crawl_delay  time\n",
      "0        1           a           20   1.0\n",
      "1        2           b           20   2.3\n",
      "2        3           d           20   2.4\n",
      "3        5           e           50   3.0\n",
      "2    d\n",
      "Name: robots_link, dtype: object\n",
      "2    d\n",
      "Name: robots_link, dtype: object\n",
      "2\n",
      "i\n",
      "   domains robots_link  crawl_delay  time\n",
      "0        1           a           20   1.0\n",
      "1        2           b           20   2.3\n",
      "2        3           i           20   2.4\n",
      "3        5           e           50   3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clothilde/.local/lib/python3.6/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-8d6cb89889c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"domains\"\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "# Test dataframe methods on Domains \n",
    "\n",
    "D = pd.DataFrame ({\n",
    "    \"domains\": [1, 2, 3, 5],\n",
    "    \"robots_link\": [\"a\", \"b\", \"d\", \"e\"],\n",
    "    \"crawl_delay\": [20, 20, 20, 50],\n",
    "    \"time\": [1, 2.3, 2.4, 3]\n",
    "})\n",
    "\n",
    "print(D.head())\n",
    "\n",
    "#if 10 not in D[\"domains\"]:\n",
    "#    pd.melt(D, [10, \"g\", 20, 3.2]) #=> Melt only with df \n",
    "#print(D.head())\n",
    "\n",
    "dom = 3\n",
    "\n",
    "curr = D.loc[D[\"domains\"] == 3]   # Makes copy of row!!! Can't modif that\n",
    "\n",
    "#i = D[D[\"domains\"] == 3].index\n",
    "#print(i)\n",
    "#D[i, 2] = 10\n",
    "#print(D.head())\n",
    "\n",
    "#r = D[D[\"domains\"] == dom]\n",
    "#print(r)\n",
    "#print(r[\"time\"])\n",
    "\n",
    "r = D[D[\"domains\"] == dom][\"robots_link\"]\n",
    "print(r)\n",
    "\n",
    "D[D[\"domains\"] == dom][\"robots_link\"] = \"h\"\n",
    "print(D[D[\"domains\"] == dom][\"robots_link\"])\n",
    "\n",
    "i = D.index[D[\"domains\"] == dom].tolist()[0]\n",
    "print(i)\n",
    "\n",
    "D.loc[i, \"robots_link\"] = \"i\"\n",
    "print(D.loc[i, \"robots_link\"])\n",
    "print(D.head())\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
