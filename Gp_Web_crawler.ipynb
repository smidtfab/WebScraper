{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse as parse\n",
    "import urllib.robotparser as rp\n",
    "\n",
    "from socket import gethostbyname, gaierror\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from socket import gethostbyname, gaierror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links as often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 80     # max nb of links to crawl\n",
    "max_cont = 10      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one function to retrieve all external links in the page (links to the same domain do not begin by htts, and as such are not spotted by Beautiful Soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        print(\"Counter = {} - max_l = {}\".format(counter, max_l))\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            save_link(link.get('href'), str(counter))\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    counter = len(Links)\n",
    "        \n",
    "    Links, counter = get_all_links (url, counter, Links, max_l)\n",
    "    \n",
    "    # If we still do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white', 'http://www.ghirardelli.com/chocopedia/tips.aspx']\n50\n"
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link (url, filename):\n",
    "    try:\n",
    "        doc = requests.get(url.strip())\n",
    "        # Put here the path where you want to save the link's contents\n",
    "        name = \"Docs_crawled4/\" + filename + \".html\"\n",
    "        with open(name, 'wb') as fOut:\n",
    "            fOut.write(doc.content)\n",
    "    except gaierror:\n",
    "        print(\"Link not reachable\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'0001'"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convention: the filename will be the url's index in the list, on 4 characters\n",
    "\n",
    "# Prepare new filename\n",
    "def new_filename (count, fname = '0000', conv = [3, 1]):\n",
    "    nb_zeros = conv[0]\n",
    "    pw = conv[1]\n",
    "    \n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count)\n",
    "    \n",
    "    return (filename, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "\n",
    "conventions = [nb_zeros, pw]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    filename, conventions = new_filename(count, filename, conventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling without considering robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler_noRespect(L = [URL], pointer=0, last_expanded = 0, fname = '0000', \\\n",
    "                      max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    url = L[pointer]\n",
    "\n",
    "    # Save contents of the link\n",
    "    save_link(url, fname)\n",
    "    max_c -= 1                 \n",
    "\n",
    "    # Prepare filename for next page\n",
    "    pointer += 1\n",
    "    fname, conv = new_filename(pointer, fname, conv)\n",
    "\n",
    "    # If we do not have enough links, save those referenced in the page\n",
    "    if (len(L) < max_l):\n",
    "        L, counter = get_all_links(url, counter, L, max_l)\n",
    "    \n",
    "    # If we are close to the end of the queue of links and have not saved enough content yet, get more \n",
    "    if (pointer >= (len(L) - 2)):\n",
    "        l = len(L)\n",
    "        while (len(L) == l):\n",
    "            last_expanded +=1\n",
    "            L, counter2 = get_all_links(L[last_expanded], 0, L, max_l)\n",
    "            counter += counter2\n",
    "\n",
    "    # If we have not saved enough links: repeat\n",
    "    if (max_c > 0):\n",
    "        L = Crawler_noRespect (L, pointer, last_expanded, fname, max_l, max_c, conv)\n",
    "    \n",
    "    return (L)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: works 100% fine\n",
    "\n",
    "L_noRobots = Crawler_noRespect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "en.wikipedia.org\nhttps://en.wikipedia.org/robots.txt\n"
    }
   ],
   "source": [
    "# Test on URL = wikipedia\n",
    "\n",
    "# Getting domain url and robots.txt link\n",
    "## Using urlparse\n",
    "URL_parsed = parse.urlparse(URL)\n",
    "print(URL_parsed.netloc)\n",
    "URL_parsed\n",
    "\n",
    "robot = URL_parsed.scheme + \"://\" + URL_parsed.netloc + \"/robots.txt\"\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://en.wikipedia.org/wiki/robots.txt\nhttp://www.eluniversal.com.mx/notas/robots.txt\n"
    }
   ],
   "source": [
    "## Using urljoin -DOESN'T WORK \n",
    "robot = parse.urljoin(URL, \"robots.txt\")\n",
    "print(robot)\n",
    "\n",
    "robot = parse.urljoin(links_crawled[2], \"robots.txt\")\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "False"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Robot parsing\n",
    "\n",
    "RP = rp.RobotFileParser()            # Define Robot parser object\n",
    "\n",
    "#robot = \"https://en.wikipedia.org/robots.txt\"\n",
    "RP.set_url(robot)                    # Set robots.txt link\n",
    "\n",
    "RP.read()                            # Open robots.txt file\n",
    "rrate = RP.request_rate(\"*\")         # Return requests per second rate (requests, seconds)\n",
    "print(RP.crawl_delay(\"*\"))           # Returns crawl delay\n",
    "\n",
    "RP.can_fetch(\"*\", URL)               # Returns bool if can crawl that url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if robots.txt exists -> doesn't work for our second link\n",
    "def does_url_exist(url):\n",
    "    try: \n",
    "        r = requests.head(url)\n",
    "        if r.status_code < 400:\n",
    "            print(1)\n",
    "            return True\n",
    "        else:\n",
    "            print(0)\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        # handle your exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function bringing it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode: Crawler (nb_max_url, [URL], Domains, Links):\n",
    "\n",
    "Get robots.txt url + domain \n",
    "Save domain, robots.txt url and crawl delay if don't already have it\n",
    "\n",
    "If can fetch: \n",
    "\n",
    "    If domain in domains & time crawled > 0:\n",
    "        Wait max(0, crawl delay - (current time - time crawled))\n",
    "    \n",
    "    Crawl: save contents \n",
    "    Update time crawled\n",
    "    Save links if we still need more\n",
    "    Save in matrix domains line: domain, crawl delay (20ms if none), time crawled\n",
    "\n",
    "If haven't crawled enough links: repeat\n",
    "    \n",
    "Links = list of links we can crawl\n",
    "Domains: [domain, robots.txt link, crawl delay, time last crawled]\n",
    "(several links can have same domain)\n",
    "\n",
    "Auxiliary functions: extract up to M links from a page, new filename, save a link (all checked individually, work)\n",
    "\n",
    "To be able to wait for crawler delay time, need functions get current time, wait a certain time (need to know units of said time!!!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function with Domain matrix as list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = []\n",
    "robots = []\n",
    "delays = []\n",
    "time_crawled = []\n",
    "\n",
    "Links = [URL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with D as list of lists (domains, robots, delay, time_crawled)\n",
    "\n",
    "def Crawler (D = [domains, robots, delays, time_crawled], L=Links, pointer=0, \\\n",
    "             fname = '0000', max_l=max_links, max_c = max_cont, conv = [3, 1]):\n",
    "    \n",
    "    counter = len(L)\n",
    "    #print(\"Nb links in Links: \", counter)\n",
    "    #print(\"Pointer: \", pointer)\n",
    "    already_crawled = True\n",
    "    \n",
    "    # Issue with some links: don't parse them        \n",
    "    #if not (pointer in [1, 4]):\n",
    "    \n",
    "    url = L[pointer]\n",
    "    print(url)\n",
    "\n",
    "    # Get url domain\n",
    "    URL_parsed = parse.urlparse(url)\n",
    "    dom = URL_parsed.netloc\n",
    "    #print(\"Domain: \", dom)\n",
    "\n",
    "    # If we have not already tried to crawl in the domain\n",
    "    if not (dom in domains):\n",
    "\n",
    "        #print(\"New domain\")\n",
    "        already_crawled = False        \n",
    "        # Get robots.txt link\n",
    "        r = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "        #print(\"Robots.txt link: \", r)\n",
    "\n",
    "        domains.append(dom)\n",
    "        robots.append(r)\n",
    "        delays.append(20)\n",
    "        time_crawled.append(-1)\n",
    "\n",
    "\n",
    "    i = domains.index(dom)\n",
    "    #print(\"Domain index: \", i)\n",
    "    r = robots[i]\n",
    "    #print(\"Robots.txt link: \", r)\n",
    "\n",
    "    # If our domain is not dead (xocoatl refuses all robots)\n",
    "    if (not dom in [\"www.cfsan.fda.gov\", \"www.cocoatree.org\", \"nca.files.cms-plus.com\", \\\n",
    "                    \"www.xocoatl.org\", \"www.cfsan.fda.gov\"]):\n",
    "        \n",
    "        # Access robots.txt\n",
    "        RP = rp.RobotFileParser()         \n",
    "        RP.set_url(r) \n",
    "        RP.read()\n",
    "\n",
    "        # If the robot parser correctly opens the robots.txt. \n",
    "        # This test is actually too harsh, but it's better to not parse some links that could be saved rather than \n",
    "        # stop the whole process (some robots.txt links actually send us directly to the home domain first, or don't exist)\n",
    "        if (len(RP.entries) != 0):\n",
    "\n",
    "            # Get crawl delay (if it is given and we don't already have it)\n",
    "            if ((not already_crawled) & (RP.crawl_delay(\"*\") != None)):\n",
    "                delays.pop(i) \n",
    "                delays.insert(i, RP.crawl_delay(\"*\"))\n",
    "\n",
    "            # If we can crawl the file\n",
    "            if (RP.can_fetch(\"*\", url)):\n",
    "\n",
    "                # Wait appropriate time if we have already crawled the domain\n",
    "                if (already_crawled):\n",
    "                    wait = max(0, delays[i] - (time.time() - time_crawled[i]))\n",
    "                    if (wait > 0): \n",
    "                               time.sleep(wait)\n",
    "\n",
    "                \n",
    "                max_c -= 1\n",
    "                print(\"\\n MAX_C - > {}\".format(max_c))\n",
    "                print(\"We are saving the contents of \", url)\n",
    "\n",
    "                # Update time we last crawled the domain\n",
    "                time_crawled.pop(i)\n",
    "                time_crawled.insert(i, time.time())                  \n",
    "\n",
    "                # Prepare filename for next page\n",
    "                fname, conv = new_filename(pointer + 1, fname, conv)\n",
    "\n",
    "                # If we do not have enough links, save those referenced in the page\n",
    "                if (len(L) < max_l):\n",
    "                    L, counter = get_all_links(L[pointer], counter, L, max_l)\n",
    "    print(D)\n",
    "    #print(L)\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (counter < max_l):\n",
    "        pointer += 1\n",
    "        print(\"COUNTER_IF = {} - max_l = {}, pointer = {}\".format(counter, max_l, pointer))\n",
    "        D, L = Crawler (D, L, pointer, fname, max_l, max_c, conv)\n",
    "    else:\n",
    "        return (D, L)\n",
    "    \n",
    "    return (D, L)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "https://en.wikipedia.org/wiki/Chocolate\n\n MAX_C - > 9\nWe are saving the contents of  https://en.wikipedia.org/wiki/Chocolate\nCounter = 1 - max_l = 80\nCounter = 2 - max_l = 80\nCounter = 3 - max_l = 80\nCounter = 4 - max_l = 80\nCounter = 5 - max_l = 80\nCounter = 6 - max_l = 80\nCounter = 7 - max_l = 80\nCounter = 8 - max_l = 80\nCounter = 9 - max_l = 80\nCounter = 10 - max_l = 80\nCounter = 11 - max_l = 80\nCounter = 12 - max_l = 80\nCounter = 13 - max_l = 80\nCounter = 14 - max_l = 80\nCounter = 15 - max_l = 80\nCounter = 16 - max_l = 80\nCounter = 17 - max_l = 80\nCounter = 18 - max_l = 80\nCounter = 19 - max_l = 80\nCounter = 20 - max_l = 80\nCounter = 21 - max_l = 80\nConnection Error\nCounter = 22 - max_l = 80\nCounter = 23 - max_l = 80\nCounter = 24 - max_l = 80\nCounter = 25 - max_l = 80\nCounter = 26 - max_l = 80\nCounter = 27 - max_l = 80\nCounter = 28 - max_l = 80\nCounter = 29 - max_l = 80\nCounter = 30 - max_l = 80\nCounter = 31 - max_l = 80\nCounter = 32 - max_l = 80\nCounter = 33 - max_l = 80\nCounter = 34 - max_l = 80\nCounter = 35 - max_l = 80\nCounter = 36 - max_l = 80\nCounter = 37 - max_l = 80\nCounter = 38 - max_l = 80\nCounter = 39 - max_l = 80\nConnection Error\nCounter = 40 - max_l = 80\nCounter = 41 - max_l = 80\nCounter = 42 - max_l = 80\nCounter = 43 - max_l = 80\nCounter = 44 - max_l = 80\nCounter = 45 - max_l = 80\nCounter = 46 - max_l = 80\nConnection Error\nCounter = 47 - max_l = 80\nCounter = 48 - max_l = 80\nCounter = 49 - max_l = 80\nCounter = 50 - max_l = 80\nCounter = 51 - max_l = 80\nCounter = 52 - max_l = 80\nCounter = 53 - max_l = 80\nCounter = 54 - max_l = 80\nCounter = 55 - max_l = 80\nCounter = 56 - max_l = 80\nCounter = 57 - max_l = 80\nCounter = 58 - max_l = 80\nCounter = 59 - max_l = 80\nCounter = 60 - max_l = 80\nCounter = 61 - max_l = 80\nCounter = 62 - max_l = 80\nCounter = 63 - max_l = 80\nCounter = 64 - max_l = 80\nCounter = 65 - max_l = 80\nConnection Error\nCounter = 66 - max_l = 80\nConnection Error\nCounter = 67 - max_l = 80\nCounter = 68 - max_l = 80\nCounter = 69 - max_l = 80\nCounter = 70 - max_l = 80\nCounter = 71 - max_l = 80\nCounter = 72 - max_l = 80\nConnection Error\nCounter = 73 - max_l = 80\nCounter = 74 - max_l = 80\nCounter = 75 - max_l = 80\nCounter = 76 - max_l = 80\nCounter = 77 - max_l = 80\nCounter = 78 - max_l = 80\n[['en.wikipedia.org'], ['https://en.wikipedia.org/robots.txt'], [20], [1584730347.8845086]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 1\nhttp://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate\n[['en.wikipedia.org', 'ndb.nal.usda.gov'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt'], [20, 20], [1584730347.8845086, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 2\nhttp://www.eluniversal.com.mx/notas/526113.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt'], [20, 20, 20], [1584730347.8845086, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 3\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt'], [20, 20, 20, 20], [1584730347.8845086, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 4\nhttp://www.bartleby.com/61/68/C0316800.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt'], [20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 5\nhttp://antiquity.ac.uk/projgall/powis/index.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt'], [20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 6\nhttp://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt'], [20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 7\nhttp://www.museum.upenn.edu/new/news/fullrelease.php?which=306\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 8\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 9\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 10\nhttp://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 11\nhttp://archive.fieldmuseum.org/chocolate/history.html\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 12\nhttp://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1, -1]]\nCOUNTER_IF = 79 - max_l = 80, pointer = 13\nhttp://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford\n\n MAX_C - > 8\nWe are saving the contents of  http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford\nCounter = 79 - max_l = 80\nCounter = 80 - max_l = 80\n[['en.wikipedia.org', 'ndb.nal.usda.gov', 'www.eluniversal.com.mx', 'archive.fieldmuseum.org', 'www.bartleby.com', 'antiquity.ac.uk', 'news.sciencemag.org', 'www.museum.upenn.edu', 'findarticles.com', 'www.newyorker.com'], ['https://en.wikipedia.org/robots.txt', 'http://ndb.nal.usda.gov/robots.txt', 'http://www.eluniversal.com.mx/robots.txt', 'http://archive.fieldmuseum.org/robots.txt', 'http://www.bartleby.com/robots.txt', 'http://antiquity.ac.uk/robots.txt', 'http://news.sciencemag.org/robots.txt', 'http://www.museum.upenn.edu/robots.txt', 'http://findarticles.com/robots.txt', 'http://www.newyorker.com/robots.txt'], [20, 20, 20, 20, 20, 20, 20, 20, 20, 20], [1584730347.8845086, -1, -1, -1, -1, -1, -1, -1, -1, 1584730607.2248638]]\n"
    }
   ],
   "source": [
    "# Emptying lists from previous tests\n",
    "for l in [domains, robots, delays, time_crawled]:\n",
    "    while len(l) > 0:\n",
    "            l.pop(0)\n",
    "\n",
    "while (len(Links) > 1):\n",
    "    Links.pop(1)\n",
    "\n",
    "# Save contents of the root link\n",
    "save_link(URL, \"000\")\n",
    "D, Links = Crawler()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}