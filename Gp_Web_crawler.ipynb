{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import urllib.request\n",
    "import urllib.parse as parse\n",
    "import urllib.robotparser as rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set an initial URL in accordance with our theme, a maximum number of links to save and a maximum number of links to download the contents of. This way, if the links we get do not want to be crawled, we do not need to go back and save more links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Chocolate'\n",
    "max_links = 50     # max nb of links to crawl\n",
    "max_cont = 20      # max nb of links to get content of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regex to extract all links from url, stop if we reach maximum\n",
    "def get_all_links (url, counter, Links = [], max_l=max_links):\n",
    "    \n",
    "    html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "    \n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        if(counter < max_l)&(link not in Links):\n",
    "            Links.append(link.get('href'))\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return (Links, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This method return at most max_l links, starting from an initial url and getting its children links, \n",
    "their children links and so on in a breadth-first fashion.\n",
    "\n",
    "Returns:\n",
    "    list\n",
    "        a list of strings that are the links\n",
    "\"\"\"\n",
    "\n",
    "def crawl_for_links(Links=[], pointer=0, max_l=max_links):\n",
    "    \n",
    "    url = Links[pointer]\n",
    "    counter = len(Links)\n",
    "        \n",
    "    Links, counter = get_all_links (url, counter, Links, max_l)\n",
    "    #html_page = urllib.request.urlopen(url)   # Open url on internet\n",
    "    #soup = bs(html_page, 'lxml')              # Open contents of url\n",
    "      \n",
    "\n",
    "    #for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "    #    if(counter < max_l)&(link not in Links):\n",
    "    #        Links.append(link.get('href'))\n",
    "    #        counter += 1\n",
    "    #    else:\n",
    "    #        break\n",
    "            \n",
    "    # If we stll do not have enough links: repeat the process with the next url from the list\n",
    "    if counter < max_l :\n",
    "        Links = crawl_for_links(Links, pointer+1, max_l - counter)\n",
    "        \n",
    "\n",
    "    return Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Chocolate', 'http://ndb.nal.usda.gov/ndb/foods/show/6153?fgcd=&manu=&lfacet=&format=&count=&max=35&offset=&sort=&qlookup=Candies%2C+milk+chocolate', 'http://www.pnas.org/content/108/21/8595', 'http://www.eluniversal.com.mx/notas/526113.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican7.html', 'http://www.bartleby.com/61/68/C0316800.html', 'http://antiquity.ac.uk/projgall/powis/index.html', 'http://news.sciencemag.org/2013/01/earliest-evidence-chocolate-north-america', 'http://www.museum.upenn.edu/new/news/fullrelease.php?which=306', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican5.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican3.html', 'http://archive.fieldmuseum.org/Chocolate/history_mesoamerican4.html', 'http://archive.fieldmuseum.org/chocolate/history.html', 'http://findarticles.com/p/articles/mi_m1310/is_1990_Jan/ai_8560999', 'http://www.newyorker.com/reporting/2007/10/29/071029fa_fact_buford', 'http://www.exploratorium.edu/exploring/exploring_chocolate/', 'http://www.smithsonianmag.com/arts-culture/a-brief-history-of-chocolate-21860917/?no-ist', 'http://hdl.handle.net/2027/spo.did2222.0000.474', 'http://www.history.com/news/hungry-history/the-sweet-history-of-chocolate', 'http://www.bartleby.com/61/68/C0316800.html', 'http://www.etymonline.com/?term=Chocolate', 'http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:32000L0036:EN:NOT', 'http://www.cfsan.fda.gov/~lrd/fr021004.html', 'http://www.worldcocoafoundation.org/wp-content/uploads/files_mf/kinta2009.pdf', 'http://www.spiritofaloha.com/features/0907/rarest.html', 'http://news.bbc.co.uk/2/hi/africa/2042474.stm', 'http://thecnnfreedomproject.blogs.cnn.com/2012/01/19/slavery-in-cocoa-fields-a-horrible-normal/', 'http://www.pantagraph.com/articles/2007/08/07/money/doc46b877b718f6e292646985.txt', 'http://www.bbc.co.uk/radio4/factual/foodprogramme_20071223.shtml', 'http://worldcocoafoundation.org/wp-content/uploads/cocoa-market-update-as-of-3.20.2012.pdf', 'http://www.dallasfood.org/modules.php?name=News&file=article&sid=78', 'http://www.businessinsider.com/when-chocolate-extinct-2017-12', 'http://www.botanical.com/botanical/mgmh/c/cacao-02.html', 'http://www.xocoatl.org/tree.htm', 'http://www.ama.ab.ca/cps/rde/xchg/ama/web/membership_WWarticle-Feb08-UpFront-10192.htm', 'http://www.metaefficient.com/food-and-drink/criollo-chocolate-efficient-food-of-the-gods.html', 'http://www.sensationalchocolates.com/ingredients.html', 'http://www.icco.org/questions/varieties.htm', 'http://www.xocoatl.org/harvest.htm', 'http://api.ning.com/files/Gr6vd-Aqjs2vj5t4p5tX-cZBB1kpTzKVDk8I59vVV6*FgB5zFYsVooD0wRRlQHO8tNMURHMwKR5hmuPX2Pe3al3iWOhcsi5S/Makingchocolatefromscratch.pdf', 'http://www.cocoatree.org/frombeantobar/atthechocolatefactory.asp', 'http://www.chocolateusa.org/pdfs/CMA-stakeholder.pdf', 'http://www.typetive.com/blogimages/07p-0085AppendixC.pdf', 'http://news.bbc.co.uk/1/hi/uk/678141.stm', 'http://laws.justice.gc.ca/eng/regulations/C.R.C.,_c._870/page-33.html#h-66', 'http://www.sallys-place.com/food/columns/zonis/conching.htm', 'http://candy.about.com/od/candybasics/ht/temperchoc.htm', 'http://nca.files.cms-plus.com/Sweet_Truth_Cocoa_Butter_WEB.pdf', 'http://www.rigb.org/assets/uploads/docs/Lecture%20two_designed.pdf', 'http://news.sciencemag.org/chemistry/2015/05/x-rays-reveal-how-chocolate-turns-white']\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "links_crawled = crawl_for_links([URL])\n",
    "print(links_crawled)\n",
    "print(len(links_crawled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link (url, filename):\n",
    "    doc = requests.get(url)\n",
    "    # Put here the path where you want to save the link's contents\n",
    "    name = \"Docs_crawled2/\" + filename + \".html\"\n",
    "    with open(name, 'wb') as fOut:\n",
    "        fOut.write(doc.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "save_link (URL, '000')\n",
    "filename = 3*'0' + '1'\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convention: the filename will be the url's index in the list, on 4 characters\n",
    "\n",
    "# Prepare new filename\n",
    "def new_filename (count, fname = '0000', conv = [3, 1]):\n",
    "    nb_zeros = conv[0]\n",
    "    pw = conv[1]\n",
    "    \n",
    "    if (count > 10**pw):\n",
    "        nb_zeros -= 1\n",
    "        pw += 1 \n",
    "    # If the file index is above the next power of 10, we add one less 0\n",
    "    # to the filename than we did before\n",
    "    filename = nb_zeros*'0' + str (count)\n",
    "    \n",
    "    return (filename, conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"0000\"\n",
    "nb_zeros = 3\n",
    "pw = 1\n",
    "\n",
    "conventions = [nb_zeros, pw]\n",
    "\n",
    "count = 0\n",
    "\n",
    "for url in links_crawled:\n",
    "    # Save the link\n",
    "    save_link(url, filename)\n",
    "    count += 1\n",
    "    # Break when we have enough links\n",
    "    if count >= max_cont : break\n",
    "        \n",
    "    filename, conventions = new_filename(count, filename, conventions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.wikipedia.org\n",
      "https://en.wikipedia.org/robots.txt\n"
     ]
    }
   ],
   "source": [
    "# Test on URL = wikipedia\n",
    "\n",
    "# Getting domain url and robots.txt link\n",
    "## Using urlparse\n",
    "URL_parsed = parse.urlparse(URL)\n",
    "print(URL_parsed.hostname)\n",
    "URL_parsed\n",
    "\n",
    "robot = URL_parsed.scheme + \"://\" + URL_parsed.netloc + \"/robots.txt\"\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/robots.txt\n",
      "http://www.pnas.org/content/108/21/robots.txt\n"
     ]
    }
   ],
   "source": [
    "## Using urljoin -DOESN'T WORK \n",
    "robot = parse.urljoin(URL, \"robots.txt\")\n",
    "print(robot)\n",
    "\n",
    "robot = parse.urljoin(links_crawled[2], \"robots.txt\")\n",
    "print(robot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Robot parsing\n",
    "\n",
    "RP = rp.RobotFileParser()            # Define Robot parser object\n",
    "\n",
    "#robot = \"https://en.wikipedia.org/robots.txt\"\n",
    "RP.set_url(robot)                    # Set robots.txt link\n",
    "\n",
    "RP.read()                            # Open robots.txt file\n",
    "rrate = RP.request_rate(\"*\")         # Return requests per second rate (requests, seconds)\n",
    "print(RP.crawl_delay(\"*\"))           # Returns crawl delay\n",
    "\n",
    "RP.can_fetch(\"*\", URL)               # Returns bool if can crawl that url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function bringing it all together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode: Crawler (nb_max_url, [URL], Domains, Links):\n",
    "\n",
    "Get robots.txt url + domain \n",
    "Save domain, robots.txt url and crawl delay if don't already have it\n",
    "\n",
    "If can fetch: \n",
    "\n",
    "    If domain in domains & time crawled > 0:\n",
    "        Wait max(0, crawl delay - (current time - time crawled))\n",
    "    \n",
    "    Crawl: save contents \n",
    "    Update time crawled\n",
    "    Save links if we still need more\n",
    "    Save in matrix domains line: domain, crawl delay (20ms if none), time crawled\n",
    "\n",
    "If haven't crawled enough links: repeat\n",
    "    \n",
    "Links = list of links we can crawl\n",
    "Domains: [domain, robots.txt link, crawl delay, time last crawled]\n",
    "(several links can have same domain)\n",
    "\n",
    "Auxiliary functions: extract up to M links from a page, new filename, save a link (all checked individually, work)\n",
    "\n",
    "To be able to wait for crawler delay time, need functions get current time, wait a certain time (need to know units of said time!!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crawler (Domains= [[]], Links=[], pointer=0, fname = '0000', max_l=max_links, max_c = max_cont):\n",
    "          \n",
    "    url = Links[pointer]\n",
    "    already_crawled = True\n",
    "    counter = len(Links)\n",
    "    \n",
    "    # Get url domain\n",
    "    URL_parsed = parse.urlparse(url)\n",
    "    dom = URL_parsed.netloc\n",
    "    \n",
    "    # If we have not already tried to crawl in the domain\n",
    "    if (dom not in Domains[, 0]):\n",
    "        already_crawled = False\n",
    "        # Get robots.txt link\n",
    "        robot = URL_parsed.scheme + \"://\" + dom + \"/robots.txt\"\n",
    "        Domains.append([dom, robot, 20, -1])\n",
    "    \n",
    "    i = Domains[, 0].index(dom)\n",
    "    robot = Domains[i, 1]\n",
    "\n",
    "    # Access robots.txt\n",
    "    RP = rp.RobotFileParser()         \n",
    "    RP.set_url(robot)                    \n",
    "    RP.read()\n",
    "    \n",
    "    # Get crawl delay (if it is given and we don't already have it)\n",
    "    if (not already_crawled) & (RP.crawl_delay(\"*\") != None):\n",
    "        Domains[i, 2] = RP.crawl_delay(\"*\")\n",
    "    \n",
    "    # If we can crawl the file\n",
    "    if (RP.can_fetch(\"*\", URL)):\n",
    "        \n",
    "        # Wait appropriate time if we have already crawled the domain\n",
    "        #if (already_crawled):\n",
    "        #    Wait max(0, crawl delay - (current time - time crawled))\n",
    "        \n",
    "        # Save contents of the link\n",
    "        save_link(url, fname)\n",
    "        # Domains[i, 3] = current_time                  # update time we last crawled the domain\n",
    "        \n",
    "        # Prepare filename for next page\n",
    "        pointer += 1\n",
    "        fname = filename(fname, pointer, conv)\n",
    "        \n",
    "        # If we do not have enough links, save those referencec in the page\n",
    "        if len(Links) < max_l:\n",
    "            Links, counter = get_all_links(url, counter, Links, max_l)\n",
    "    \n",
    "    # If we have not saved enough links: repeat\n",
    "    if (pointer < max_c):\n",
    "        Domains, Links = Crawler (Domains, Links, pointer, fname, max(0, max_l - counter), max_c - pointer)\n",
    "        \n",
    "    return (Domains, Links)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "2 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-198ffcc11c8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 2 is not in list"
     ]
    }
   ],
   "source": [
    "# Test list methods on Domains\n",
    "\n",
    "D = [[1, \"a\", 20, 1], [2, \"b\", 20, 2], [3, \"c\", 20, 4]]\n",
    "\n",
    "print(D.index(2))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
