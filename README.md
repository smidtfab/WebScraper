# WebScraper

Current file: Web_crawler, old copy: SearchEngine
File to send: Clean_WebCrawler. Only edit the last two cells with the definition of the Crawler and its test, by
copying from the Web_crawler file once everything works.

The links from the crawler that does not consider the robot are saved into Docs_crawled_noRobot.
Those saved from the "full" crawler are saved in Docs_crawled4.

Note that links are saved inside folder Documents_craled in the crawler's repository, here saved inside data folder.

GitHub for urllib-robotparser
https://github.com/python/cpython/blob/3.8/Lib/urllib/robotparser.py

Look at https://blog.hartleybrody.com/web-scraping-cheat-sheet/ for general help
